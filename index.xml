<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tullia Padellini</title>
    <link>/</link>
    <description>Recent content on Tullia Padellini</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Nov 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The almighty Bootstrap</title>
      <link>/post_ita/2018-04-29-bootstrap/</link>
      <pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post_ita/2018-04-29-bootstrap/</guid>
      <description>Ingredienti di base Riprendiamo il classico setup visto ad inferenza, ossia supponiamo di avere un campione \(X_1,\dots X_n\) di variabili indipendenti ed identicamente distribuite (i.i.d.) proveniente da una distribuzione \(F\), dove \(F(x) = \mathbb{P}(X_i\leq x)\).
Indichiamo con \(\mu_F\) e \(\sigma^2 _F\) rispettivamente la media e la varianza della generica variabile \(X_i\). La notazione con \(F\) sottoscritto serve a ricordare che sia \(\mu_F\) che \(\sigma^2_F\) sono funzioni della funzione di ripartizione, infatti:</description>
    </item>
    
    <item>
      <title>R - 101</title>
      <link>/post_ita/lab01/</link>
      <pubDate>Thu, 13 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post_ita/lab01/</guid>
      <description>Introduzione a R Queste note costituiscono un’introduzione ad R, un linguaggio di programmazione rivolto all’analisi dei dati, utilizzato in statistica e data mining. Perchè proprio R?
 È gratuito: contrariamente ad altri software comunemente utilizzati in ambito economico-statistico (ad esempio SAS, STATA etc), R ? open-source. È aggiornato: quando viene presentata una nuova metodologia di analisi dei dati, molto spesso la prima implementazione ? proprio in R. È redditizio…  Informazioni introduttive possono essere trovate su wikipedia e r-project.</description>
    </item>
    
    <item>
      <title>Basic Monte Carlo</title>
      <link>/post_en/montecarlo/</link>
      <pubDate>Wed, 12 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post_en/montecarlo/</guid>
      <description>The intuition: Remember the (weak) Law of Large Numbers? Just a quick recap: if \(X_1,\dots, X_n\) are i.i.d. random variables from a distribution \(F\) with mean \(\mu\) and variance \(\sigma^2\), then the sample mean \(\bar{X}_n\) converges in probability to the population mean \(\mu\).
Using more formulas, we can write the LLN as \[ \bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \rightarrow \mu =\int xdF(x)\] as \(n\rightarrow \infty\). One way to look at this is that, if the sample size \(n\) is “close to infinity” (or more generally, if \(n\) is “large enough”), then we have \[\frac{1}{n}\sum_{i=1}^n X_i \approx \int xdF.</description>
    </item>
    
    <item>
      <title></title>
      <link>/about/</link>
      <pubDate>Thu, 01 Jan 1970 01:33:39 +0100</pubDate>
      
      <guid>/about/</guid>
      <description>img { display: block; margin-left: auto; margin-right: auto; }  education @ Liceo Classico Tito Lucrezio Caro:
 2010 &amp;ndash; Maturità Classica  
@ Sapienza Università di Roma:
 2013 &amp;ndash; Bachelor in Statistics and Economics 2015 &amp;ndash; Master in Statistics and Decision Sciences (EuroBayes program) 2019 &amp;ndash; PhD in Metodological Statistics  
out of school I live for boring documentaries, trash music, junk food &amp;amp; fancy ice-cream.</description>
    </item>
    
    <item>
      <title></title>
      <link>/google887f8702c0c8fb10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/google887f8702c0c8fb10/</guid>
      <description>google-site-verification: google887f8702c0c8fb10.</description>
    </item>
    
    <item>
      <title></title>
      <link>/teaching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/teaching/</guid>
      <description>This semester I am tutoring for the bachelor course of Statistics and I am in charge of the Digital Skills - R course, both at LUISS.
&amp;nbsp;
I help out here and there with courses of the DSS, Statistical Learning above all.
&amp;nbsp; &amp;nbsp;
where &amp;amp; when to find me:  address: stanza 13 &amp;ndash; Dipartimento di Scienze Statistiche, Piazzale Aldo Moro 5, 00185, Rome, ITALY office hours:  @Sapienza: tuesday 9 &amp;ndash; 11 @LUISS: friday 11 &amp;ndash; 12   &amp;nbsp; &amp;nbsp;</description>
    </item>
    
    <item>
      <title>Blog</title>
      <link>/blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/blog/</guid>
      <description>Together with two smarter &amp;amp; prettier (but equally nerd) guys, I tried blogging.
&amp;nbsp; &amp;nbsp;
Our original plan was to use it to procrastinate the real work we were supposed to do, but we quickly became such pros that we managed to procrastinate procrastination. It will be updated at some point.</description>
    </item>
    
    <item>
      <title>Code</title>
      <link>/code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/code/</guid>
      <description>I am the proud mantainer of the following R-packages:

kernelTDA with Francesco Palini &amp;amp; Pierpaolo Brutti
Tools for Statistical Learning with Topological Summaries. It focuses on Kernel methods for Topological Data Analysis, implementing the most famous Kernel functions defined on the space of Persistence Diagrams and a solver for SVM with indefinite kernels (or SVM in Reproducible Kernel Krein Space) to exploit them. Additionally it provides an R interface to the C++ library HERA, which allows to efficently compute Wasserstein distances between Persistence Diagrams.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/research/</guid>
      <description>My two (main) topics of interest are Quantile Regression and Topological Data Analysis.
During my PhD this was materialized in what follows:  Model-based Quantile Regression for Discrete Data
 Supervised Learning with Indefinite Topological Kernels
 Persistence Flamelets: multiscale Persistent Homology for kernel density exploration
  
As of now I am working on
 Regression trees for mixed type data (including Topological summaries!)
 Wasserstein distance for sample size determination, with applications to clinical trials</description>
    </item>
    
    <item>
      <title>Talks</title>
      <link>/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/talks/</guid>
      <description>[2019]
CRONOS meeting &amp;ndash; April, Limassol (CY) [talk]
Topological low dimensional learning of high dimensional time series
&amp;nbsp; &amp;nbsp;
[2018]
CMS-Statistics &amp;ndash; 14-16 December, Pisa (IT) [talk]
Topological Invariants for high-dimensional Time Series
&amp;nbsp;
KAUST Statistics Workshop &amp;ndash; November, Thuwal (KSA) [poster]
Predicting Dengue Epidemics in Brazil using Quantile Regression
&amp;nbsp;
ISBA World Meeting &amp;ndash; June, Edinburgh (UK) [poster]
Bayesian Quantile Regression for Discrete Data
&amp;nbsp;
Riunione Scientifica SIS &amp;ndash; June, Palermo (IT) [talk]</description>
    </item>
    
  </channel>
</rss>