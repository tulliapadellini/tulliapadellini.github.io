<!DOCTYPE html>
<html lang="en-us">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Tullia Padellini">
    <meta name="description" content="Research Associate in Spatial Statistics and Epidemiology at Imperial College London">
    <meta name="keywords" content="[still very much under construction]">

    <base href="/">
    <title>
  Basic Monte Carlo · Tullia Padellini
</title>

    <link rel="canonical" href="/post_en/montecarlo/">

    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="//cdn.rawgit.com/necolas/normalize.css/master/normalize.css">
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="icon" type="image/png" href="/images/coding-2.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/coding.png" sizes="16x16">

    

    <meta name="generator" content="Hugo 0.59.1" />
  </head>

  <body>
    <main class="wrapper">
      <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/github-gist.css" rel="stylesheet" id="theme-stylesheet">
<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Tullia Padellini
    </a>
    
    <ul class="navigation-list float-right">
      
      <li class="navigation-item">
        <a class="navigation-link" href="/blog/">Blog</a>
      </li>
      
      <li class="navigation-item">
        <a class="navigation-link" href="/about/">About</a>
      </li>
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Basic Monte Carlo</h1>
    </header>

    


<div id="the-intuition" class="section level2">
<h2>The intuition:</h2>
<p>Remember the (weak) Law of Large Numbers? Just a quick recap: if <span class="math inline">\(X_1,\dots, X_n\)</span> are i.i.d. random variables from a distribution <span class="math inline">\(F\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the sample mean <span class="math inline">\(\bar{X}_n\)</span> converges in probability to the population mean <span class="math inline">\(\mu\)</span>.</p>
<p>Using more formulas, we can write the LLN as <span class="math display">\[ \bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i  \rightarrow \mu =\int xdF(x)\]</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. One way to look at this is that, if the sample size <span class="math inline">\(n\)</span> is “close to infinity” (or more generally, if <span class="math inline">\(n\)</span> is “large enough”), then we have <span class="math display">\[\frac{1}{n}\sum_{i=1}^n X_i \approx \int xdF. \]</span></p>
<p>This means that, if we are able to simulate an arbitrarily large amount of observations <span class="math inline">\(X_i\)</span> from <span class="math inline">\(F\)</span>, then we can approximate arbitrarily well the mean <span class="math inline">\(\mu\)</span> of the distribution just by sample average.</p>
<p>But this is not all! The best part is that by the LLS (Law of the Lazy Statistician) we also have that <span class="math display">\[\frac{1}{n}\sum_{i=1}^n h(X_i) \approx \int h(x)dF .\]</span>
In some sense, we are saying that <strong>if we are able to sample from the distribution <span class="math inline">\(F\)</span>, we can approximate arbitrarily complex integrals</strong>.</p>
<p>You may not see it straight away but this has <strong>huge</strong> consequences, such as: <em>you may not ever have to solve an integral analytically again</em>… Easy to see why LLN is an important result, right?</p>
<p>Using simulations from known random variables to approximate integrals is the core concept of the so-called <strong>Monte Carlo methods</strong>. In general, Monte Carlo methods are numerical techniques which rely on random sampling to approximate their results.</p>
<p>Incidentally, Monte Carlo <a href="http://www.nowozin.net/sebastian/blog/history-of-monte-carlo-methods-part-1.html">has a pretty interesting history</a>, and dates back to von Neumann and the Manhattan Project (although some claims that the first Monte Carlo was used to solve the problem of <a href="https://en.wikipedia.org/wiki/Buffon%27s_needle">Buffon’s needle</a> almost <span class="math inline">\(200\)</span> years before).</p>
</div>
<div id="standard-integrals" class="section level2">
<h2>Standard Integrals</h2>
<p>Monte Carlo is first and foremost one way of approximating integrals that we cannot (or do not want to) solve analytically. Consider for example: <span class="math display">\[ \int^{2.5} _1 \frac{\sec x + \tan x}{(\sec x -\tan x)(4\tan ^2 x)^2} 8 \sec ^2 x d x. \]</span></p>
<p>If we can write this integral as an Expected Value then we can solve it using Monte Carlo. In actual fact it is pretty easy to do it.</p>
<p>Remember that the density <span class="math inline">\(f_U(x)\)</span> of a continuous uniform random variable <span class="math inline">\(U\sim(a,b)\)</span> is <span class="math display">\[f_U (x) = \begin{cases} \frac{1}{b-a} &amp; x\in[a,b]\\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<p>For <span class="math inline">\(a =1\)</span> and <span class="math inline">\(b=2.5\)</span> we have:
<span class="math display">\[f_U (x) = \begin{cases} \frac{1}{1.5} &amp; x\in[1,2.5]\\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<p>This is enough to write the previous integral as an expectation:
<span class="math display">\[ \int^{2.5} _1 \frac{\sec x + \tan x}{(\sec x -\tan x)(4\tan ^2 x)^2} 8 \sec ^2 x \cdot 1\cdot d x =\]</span>
<span class="math display">\[ = \int^{2.5} _1 \frac{\sec x + \tan x}{(\sec x -\tan x)(4\tan ^2 x)^2} 8 \sec ^2 x \cdot \frac{1.5}{1.5}d x = \]</span>
<span class="math display">\[ = \int^{2.5} _1 \frac{\sec x + \tan x}{(\sec x -\tan x)(4\tan ^2 x)^2} 8 \sec ^2 x \cdot 1.5 f_U(x)d x = \]</span>
<span class="math display">\[  = \int^{2.5} _1 \frac{\sec x + \tan x}{(\sec x -\tan x)(4\tan ^2 x)^2} 8 \sec ^2 x 1.5 d F_U(x) =\]</span>
<span class="math display">\[  = \int^{2.5} _1 h(x) d F_U(x) = \mathbb{E}_{F_U}[h(X)] \]</span></p>
<pre class="r"><code>h.funct = function(x){
  num   = 1/cos(x) + tan(x)
  denom = (1/cos(x) - tan(x))*(4*tan(x)^2)^2
  num/denom * 8 * 1/cos(x) * 1.5
}</code></pre>
<pre class="r"><code>h.funct(runif(1,1,2.5))</code></pre>
<pre><code>## [1] 0.2030125</code></pre>
<p>This function returns one observation from the random variable <span class="math inline">\(Y = h(X)\)</span>.</p>
<p>If we repeat the procedure <span class="math inline">\(n\)</span> times, we get a sample <span class="math inline">\(Y_1, \dots, Y_n\)</span> from the distribution of <span class="math inline">\(Y\)</span>.</p>
<pre class="r"><code>n = 10000 
y.sample  = replicate(n, h.funct(runif(1, 1, 2.5)))
y.average = mean(y.sample)
y.average</code></pre>
<pre><code>## [1] -1.459875</code></pre>
</div>
<div id="strange-transformations-of-random-variables" class="section level2">
<h2>“Strange” Transformations of Random Variables</h2>
<p>Suppose we are interested in the random variable <span class="math display">\[Y = \min \left\{k \text{  such that  }\sum_{i=1}^k u_i&lt;1, \quad u_i \sim U(0,1)\right\} \]</span> in other words the variable defined as the minimum number <span class="math inline">\(k\)</span> such that <span class="math inline">\(\sum_{i=1} ^k u_i &lt; 1\)</span>, where <span class="math inline">\(u_1, \dots, u_k\)</span> are i.i.d. random variables from a <span class="math inline">\(Unif(0,1)\)</span>. This is nothing but a function of a uniform random variable, so we can write it as <span class="math inline">\(h(X)\)</span>, where <span class="math inline">\(X\sim Unif(0,1)\)</span>.</p>
<p>We are interested in the expected value of <span class="math inline">\(Y\)</span>, and it is difficult to compute it analytically. However if we use simulations, things become a lot easier.</p>
<pre class="r"><code>h.funct = function(){
  s = 0
  k = 0
  while( s &lt; 1){
    u = runif(1)
    s = s+u
    k = k+1
    }
  return(k)
}</code></pre>
<p>The function returns one observation simulated from the random variable <span class="math inline">\(Y\)</span>. If we replicate the procedure <span class="math inline">\(n\)</span> times, we get a sample <span class="math inline">\(Y_1,\dots,Y_n\)</span> distributed as <span class="math inline">\(Y\)</span>.</p>
<pre class="r"><code>n = 10000
y.sample  = replicate(n, h.funct() ) </code></pre>
<p>We can use the sample average of this new sample to estimate the population mean of the variable <span class="math inline">\(Y\)</span>.</p>
<pre class="r"><code>y.average = mean(y.sample)
y.average</code></pre>
<pre><code>## [1] 2.7133</code></pre>
<p><strong>Little side remark:</strong> Does it remind you of anything? It turns out that <span class="math inline">\(\mathbb{E}[Y]=e\)</span>, so this is just another way to define Euler’s number.</p>
<p>This is good already, but we can do more! We have an entire sample <span class="math inline">\(Y_1,\dots,Y_n\)</span> generated from the unknown distribution of <span class="math inline">\(Y\)</span>, which we can use to carry on any kind of inference procedure (which is something useless at this point of the course but please keep it in mind, it will be extremely important later on!). As for now the only thing you can do is to analyse the distribution of <span class="math inline">\(Y\)</span> looking at the histogram:</p>
<pre class="r"><code>hist(y.sample, col=rgb(.6,0,.4,.3))</code></pre>
<p><img src="/post_en/montecarlo_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="monte-carlo-for-the-coupon-problem" class="section level2">
<h2>Monte Carlo for the Coupon Problem</h2>
<p>As our last example, let us consider an exercise of basic probability, such as those you had to solve before Test::01.</p>
<p>Remember Pokemon Go? Old stuff, time flies…</p>
<p>The idea behind the game is easy:</p>
<ol style="list-style-type: decimal">
<li>walk around</li>
<li>encounter a Pokemon at random</li>
<li>catch it</li>
</ol>
<p>and then iterate the procedure until you had all of the 145 available Pokemon.</p>
<p>Different Pokemon have different probability of appearing, so you could meet the same common Pokemon many many times before meeting a new rare one. Typically you would end up with a lot of Pidgeys and no Snorlaxes, and it might take a while to complete your pokedex (i.e. catch all the Pokemon). Before starting this battery-draining adventure, you might want to know: how long will it take to catch one for each kind of Pokemon? How many Pokemon do I need to encounter in order to end the game?</p>
<p>It turns out that this is a well known probability problem, so much so that it has its own name: <strong>the Coupon problem</strong>.</p>
<p>Let us rewrite it a more rigorous way, and let us simplify the problem a little bit by assuming that all Pokemon appear with the same probability.
Denote by <span class="math inline">\(X_i\)</span> the number of encounters it takes for the player to find the <span class="math inline">\(i-th\)</span> new Pokemon after the <span class="math inline">\((i-1)\)</span>-th Pokemon has been caught. Clearly <span class="math inline">\(\mathbb{E}[X_1] =1\)</span> because the player starts with no Pokemon. After the <span class="math inline">\((i-1)\)</span>-th Pokemon has been caught, there are <span class="math inline">\(145 - (i-1)\)</span> possible Pokemon that could be the new <span class="math inline">\(i\)</span>-th Pokemon. We can interpret the process of waiting for the new <span class="math inline">\(i\)</span>-th Pokemon as a geometric distribution, where each trial is meeting a Pokemon, the “success” is getting any of the <span class="math inline">\(n-(i-1)\)</span> unobserved Pokemon and “failure” is getting a duplicate of something we already have.</p>
<p>We can thus assume that <span class="math inline">\(X_i- 1 \sim Geom(p_i)\)</span>, and use properties of the distribution to find that <span class="math inline">\(\mathbb{E}[X_i] =\frac{1}{p_i}\)</span>.</p>
<p>Remember, we are interested in the number of encounters we need to capture all Pokemon <span class="math inline">\(Y\)</span>, which is the sum of the <span class="math inline">\(X_i\)</span>.
In theory you could solve this problem with the tools that you already have (+ pen and paper), in fact, by linearity of the expected value, the problem consists in computing a bunch of expected values for Geometric distributions, i.e. <span class="math display">\[ \mathbb{E}[Y] = \mathbb{E}[X_1 +\dots +X_{145}] = \mathbb{E}[X_1] + \dots \mathbb{E}[X_{145}].\]</span></p>
<p>The “tricky” part here is how to compute the parameter <span class="math inline">\(p_i\)</span> of the Geometric distribution. If we assume that all the Pokemon have the same probability of appearing we have that, the probability of meeting the <span class="math inline">\(i\)</span>-th new Pokemon, given that you already met <span class="math inline">\((i-1)\)</span> distinct Pokemon is <span class="math display">\[p_i=\frac{145-(i-1)}{145}.\]</span>
In practice however, if we remove the oversimplifying assumption that all Pokemon have the same probability of appearing, computing this probability becomes rather long and tedious (although it is possible). Monte Carlo provides a fast (and almost brainless!) alternative.
First thing first: load the data.</p>
<pre class="r"><code>library(readr)
spawn &lt;- read_csv(&quot;spawn-by-typ.csv&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   pokemon_id = col_double(),
##   appeareances = col_double()
## )</code></pre>
<p>Then define the sample space and the probability of encountering a Pokemon:</p>
<pre class="r"><code>pokemon_sample_space = spawn$pokemon_id
spawn_prob = spawn$appeareances</code></pre>
<p>Define a function that represent the encounter with a random Pokemon:</p>
<pre class="r"><code>find.pokemon = function(){
  sample(pokemon_sample_space, 1, replace=TRUE, prob = spawn_prob)
}

sum(spawn_prob)</code></pre>
<pre><code>## [1] 598427</code></pre>
<p><strong>Warning</strong>: <code>spawn_prob</code> is not a proper probability distribution, but it does not matter because <code>sample</code> standardizes it automatically.</p>
<p>We finally define a function that will give us the number of Pokemon that we need to meet before we have a number of distinct Pokemon equal to <code>num_distinct_poke</code>. This is our function <span class="math inline">\(h\)</span>.</p>
<pre class="r"><code>simulate.catch.them.all &lt;- function(num_distinct_poke){
  captured &lt;- c()
  while(length(unique(captured)) &lt; num_distinct_poke){
    captured &lt;- c(captured, find.pokemon())
  }
  length(captured)
}</code></pre>
<p>Exactly as in the previous case, this function returns a random draw from the distribution of <span class="math inline">\(Y\)</span>; if we want to build the sample average we need, well, a sample, so we iterate the procedure.</p>
<pre class="r"><code>n = 1000
y.sample  = replicate(n,simulate.catch.them.all(10))
y.average = mean(y.sample)
y.average</code></pre>
<pre><code>## [1] 14.192</code></pre>
<p>This means that in order to get 10 different Pokemon we need to run into 14.192 Pokemon.</p>
</div>

  </article>
</section>


      </div>

      <script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<footer class="footer">
  <section class="container">

  <font size=1> © 2019 - Powered by <a href="https://gohugo.io/">Hugo</a> with much help from <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.</font>
    
  
    
  </section>
</footer>

    </main>

    

  </body>

</html>
