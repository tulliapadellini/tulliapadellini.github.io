[{"content":"   .column { float: left; } .left { width: 30%; } .right { width:70%; }  \nStatistician, Business Survey Unit\nStatistical Analysis Directorate\nDG Economics, Statistics and Research\nBank of Italy\n   \n .row { display: flex; } .column { flex: 50%; }  -- -- -- -- -- -- -- -- -- -- -- -- -- ","href":"/","title":"Home"},{"content":"I am part of the Business Survey Unit of the Bank of Italy, where I have been working since November 2021.\nIn my past life, I was a Post Doctoral Research Associate in the Environment and Health Statistics Group of Imperial College London, where I was working with Marta Blangiardo.\nWhile at Imperial, I have also been part of the Statistical Modelling Laboratory set up by the Royal Statistical Society and the Alan Turing Institute to support the UK Health Security Agency (UKHSA) COVID-19 response.\nEducation Liceo Classico Tito Lucrezio Caro: 2010 \u0026ndash; Maturità Classica\nSapienza Università di Roma: 2013 \u0026ndash; Bachelor in Statistics and Economics\n2015 \u0026ndash; Master in Statistics and Decision Sciences (EuroBayes program)\n2019 \u0026ndash; PhD in Metodological Statistics\nPersonal Miscellanea I like cardamom, boring documentaries, concrete buildings, trashy songs whose lyrics I cannot understand, weird coffees \u0026amp; niche religions.\nI spent some time in Finland (in the winter) and Saudi Arabia (in the summer), then moved to the country where talking about the weather is a national sport.\nI am a real life Tangela.\n","href":"/about/","title":"About"},{"content":"Here is a paragraph. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nHeading 2 Another one. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nHeading 4  First item Second item   Nested unordered item  Third item Nested ordered item 1 Nested ordered item 2  Heading 5 Where are the quotes!!!\n Simplify, then add lightness.\n— Colin Chapman\n Now, time for some links!\n GoHugo Hugo Themes  Heading 6 Inline code: echo \u0026quot;What is the meaning of life?\u0026quot;. Who knows?\n// Codeblock  var meaningOfLife = 42; console.log(\u0026#39;The meaning of life is: \u0026#39;, meaningOfLife);  Who wants some table?\n   Minimo Caption More Caption     Cool What? Now, wut?!    Ah, enough for today, eh?\n","href":"/typography/","title":"Typography"},{"content":"Most of the code I wrote for my analyses can be found on github.\n  Covid and Equalities \u0026ndash; this repository contains code to run analyses described in Time varying association between deprivation, ethnicity and SARS-CoV-2 infections in England: an ecological space-time study.\n  Quantile Regression \u0026ndash; this vignette illustrates how to implement the models described in Model aware quantile regression for discrete data with a focus on disease mapping.\n  In addition, I am creator and maintainer of the following R packages:\nkernelTDA with Francesco Palini \u0026amp; Pierpaolo Brutti\nTools for Statistical Learning with Topological Summaries. It focuses on Kernel methods for Topological Data Analysis, implementing the most famous Kernel functions defined on the space of Persistence Diagrams and a solver for SVM with indefinite kernels (or SVM in Reproducible Kernel Krein Space) to exploit them. Additionally it provides an R interface to the C++ library HERA, which allows to efficently compute Wasserstein distances between Persistence Diagrams.\nMore Details on the official CRAN webpage and/or in the vignette\npflamelet Persistence Flamelets, a functional summary of Multiscale Persistent Homology, and make inference on them. It implements all the methods introduced in Persistence Flamelets: multiscale Persistent Homology for kernel density exploration plus bonus features such as building bootstrap confidence bands on the Flamelet and cleaning it from noise.\nrainbaud with Alessandro Casa\nTransforms your arbitrarily long text files into color-palettes. It implements Latent Dirichlet Allocation for topic modeling to summarize long text and it provides color aggregation and color sorting methods.\nsapthesisdown Adaptation of the thesisdown package to the LaTeX template sapthesis. It contains reference styles for Bachelor, Master and PhD theses in both English and Italian.\n","href":"/code/","title":"Code"},{"content":"My fear of commitment resulted in me studying many unrelated things. An updated list of publications and working papers can be found on my scholar page.\nCOVID-19 COVID-19 Infections and health equalities with Marta Blangiardo, Radka Jersakova, Brieuc Lehmann, George Nicholson, Peter Diggle, Sylvia Richardson, Chris Holmes (\u0026amp; more)\nEthnically diverse and socio-economically deprived communities have been differentially affected by the COVID-19 pandemic in the UK. In this work we investigated the impact of such inequalities by assessing the effect of area level deprivation and ethnicity on two different metrics of COVID-19 burden, test positivity and prevalence. These two metrics have different interpretation and can be used to understand the monitoring and the spread of the infections, and together they provide a more comprehensive picture of the status of the pandemic.\nUsing a multilevel regression model we assess the time-varying association between SARS-CoV-2 infections and areal level deprivation and ethnicity. We separately consider weekly test positivity rate and estimated unbiased prevalence at the Lower Tier Local Authority (LTLA) level, adjusting for confounders and spatio-temporal correlation structure. Comparing the least deprived and predominantly White areas with most deprived and predominantly non-White areas over the whole study period, the weekly positivity rate increases by 13% from 2.97% to 3.35%. Similarly, prevalence increases by 10% from 0.37% to 0.41%. Deprivation has a stronger effect until October 2020, while the effect of ethnicity becomes more pronounced at the peak of the second wave and then again in May-June 2021. In the second wave of the pandemic, LTLAs with large South Asian populations were the most affected, whereas areas with large Black populations did not show increased values for either outcome during the entire period under analysis.\nIMD and BAME% are both associated with an increased COVID-19 burden in terms of disease spread and monitoring, and the strength of association varies over the course of the pandemic. The consistency of results across the two outcomes suggests that deprivation and ethnicity have a differential impact on disease exposure or susceptibility rather than testing access and habits\n Time varying association between deprivation, ethnicity and SARS-CoV-2 infections in England: a space-time study  COVID-19 Mortality and air-pollution with Garyfallos Konstantinoudis, Marta Blangiardo, Bethan Davies, James Bennet and Majid Ezzati\nAs there has been much discussion on the role played by air-pollution on COVID19 outcomes, we examined the effect of long term exposure to PM2.5 and NO2 on COVID19 deaths in England. As opposed to previous studies (in and outside the UK) that were based on large spatial units and critically neglected the high spatial variability of pollutants (and resulted in poor adjustment for confounders), we focused on very high resolution geographical units, the 32.844 Lower level super output areas (LSOAs) of England. We downscaled deaths data available at coarser geographical resolution matching the age-sex-ethincity structure of the deaths to that of the smaller areas, and we performed the analysis using a log linear model. In addition to adjusting for confounders related to socio-economic status, metereology, disease progression, health status, we included a random effect in order to account for the spatial autocorrelation of the data.\nOur result suggest that while there is a clear effect of long term exposure to airpollution on covid mortality in the unadjusted model, when we include the confounders the effect become negligible (and not statistically significant).\n Long-term exposure to air-pollution and COVID-19 mortality in England: a hierarchical spatial analysis  As part of the RSS-Turing Laboratory   Local prevalence of transmissible SARS-CoV-2 infection: an integrative causal model for debiasing fine-scale targeted testing data\n  Interoperability of statistical models in pandemic preparedness: principles and reality\n  Quantile Regression for Discrete Data with Haavard Rue\nQuantile regression is concerned with modeling the conditional quantile of the response distribution as opposed to the more common conditional mean. This is especially of interest when we are interested in the determinants of extreme values of the phenomenon of interest.\nIn a Bayesian framework, Quantile regression models have typically been fitted exploiting a working likelihood rather than the true generating one. We explore the case when a generative model is available for the response variable and quantile regression can be considered as an extension of GLM where the link function connects the linear predictor to the quantiles of interest rather than to the conditional mean.\nFor discrete distributions like the Poission, Binomial and the Negative Binomial, however, this approach is not viable. It is not possible to link the linear predictor directly to the quantile of interest, since the quantile function is not bijective. We argue to use a continuous model-aware interpolation of the quantile function, allowing for proper quantile inference while retaining model interpretation.\nAmong the advantages of our approach, we stress that it allows for proper uncertainty quantification and it mitigates the issue of quantile crossing. Moreover we show how our formulation makes it easy to extend quantile regression method to the context of disease mapping.\n Model-aware Quantile Regression for Discrete Data  Topological Data Analysis with [Pierpaolo Brutti]\nTopological data analysis (TDA) is a new and expanding branch of statistics devoted to recovering the shape of the data in terms of connectivity structure. As it describes very complex objects using easily interpretable features such as loops and voids, TDA has shown to be a useful way to characterize data. From a statistical standpoint, a characterization based on connectivity yields relevant information: connected components correspond to clusters, loops represent periodic structures while higher dimensional voids encode more complex dependencies.\nThere are two different problems to be tackled in TDA: learning the topology and learning with topology. In the first case we are interested in the unknown topological structure per se, as it may be critical in understanding the underlying mechanism generating the data. In the second class of problems we investigate whether topological features can be used alternatively or jointly with more traditional summaries of the data in order to improve performance of statistical methods.\nLearning with topology: Supervised Learning with Kernels Since topological summaries, most noticeably the Persistence Diagram, are typically defined in complex spaces, we adopt a kernel approach to translate them into more familiar vector spaces. We define a topological exponential kernel, we characterize it, and we show that, despite not being positive semi-definite, it can be successfully used in regression and classification tasks.\n Supervised Learning with Indefinite Topological Kernels  Learning the topology: Multiscale Persistent Homology We focus on the case of continuously varying families of objects, which comprises most of the situations where our object of interest is available at multiple resolutions, such as multidimensional time series (available at multiple time point) or kernel density estimators (that vary depending on the bandwidth parameter). This is a challenging yet common framework in statistics, where scale dependent tools arealready almost ubiquitous (smoothers being the most trivial example of it), and are destined to become even more frequent. We define thePersistence Flamelet, a multiscale version of one of the most populartool in TDA, the Persistence Landscape. We examine its theoretical properties and we showits performance as both an exploratory and inferential tool.\n Persistence Flamelets: multiscale Persistent Homology for kernel density exploration  Other Projects  Sparse Canonical Correlation analysis Wasserstein Consensus for sample size determination Object oriented conditional inference trees  ","href":"/research/","title":"Research"},{"content":"An updated list of publications and working papers can be found on my scholar page.\nAs a result of a crippling fear of commitment, I worked on many unrelated things. My two main focus however have been non parametric statistics (topological data analysis in particular) and bayesian spatio-temporal modeling (with and without applications to public health).\nBayesian spatio-temporal models for COVID-19 During my post-doc at Imperial, I worked on the development of spatio-temporal models for different metrics of COVID-19 burden (test positivity, prevalence and mortality). In particular, we tried to assess how socio-economic deprivation and air pollution were related to the spread and severity of COVID-19, and quantify the impact of COVID waves on different ethnic minorities in England.\n COVID-19 Infections and health equalities  Padellini, T., Jersakova, R., Diggle, P. J., Holmes, C., King, R. E., Lehmann, B. C., \u0026hellip; \u0026amp; Blangiardo, M. (2022). Time varying association between deprivation, ethnicity and SARS-CoV-2 infections in England: A population-based ecological study. The Lancet Regional Health-Europe. link\n COVID-19 Mortality and air-pollution  Konstantinoudis, G., Padellini, T., Bennett, J., Davies, B., Ezzati, M., \u0026amp; Blangiardo, M. (2021). Long-term exposure to air-pollution and COVID-19 mortality in England: a hierarchical spatial analysis. Environment international. link\n Prevalence Estimation  Nicholson, G., Lehmann, B., Padellini, T., Pouwels, K. B., Jersakova, R., Lomax, J., \u0026hellip; \u0026amp; Holmes, C. (2022). Improving local prevalence estimates of SARS-CoV-2 infections using a causal debiasing framework. Nature Microbiology. link\n Combining spatio-temporal models  Nicholson, G., Blangiardo, M., Briers, M., Diggle, P. J., Fjelde, T. E., Ge, H., \u0026hellip; \u0026amp; Richardson, S. (2022). Interoperability of statistical models in pandemic preparedness: principles and reality. Statistical science. link\nTopological Data Analysis The main topic of my PhD was Topological data analysis (TDA), a set of non parametric methods for describing objects through their topological features, taken to be interpretable summaries of complex dependencies. We tried to tackle two different problems: learning the topology (as the topological structure of data may be critical in understanding the underlying generating mechanism) and learning with topology (investigating whether topological features can be used alternatively or jointly with more traditional summaries of the data in order to improve performance of statistical methods).\n Learning the topology  Padellini, T., \u0026amp; Brutti, P. (2022). Persistence Flamelets: Topological Invariants for Scale Spaces. Journal of Computational and Graphical Statistics. link\n Learning with topology  Padellini, T., \u0026amp; Brutti, P. (2021). Supervised learning with indefinite topological Kernels. Statistics. link\nOther projects  Quantile Regression for Discrete Data, with Haavard Rue working paper   Sparse Canonical Correlation Analysis, with Lavinia Amorosi working paper   Bayesian modelling of administrative sources for small area population estimation, with Emily Peterson \u0026amp; many others working paper   Wasserstein Consensus for sample size determination Object oriented conditional inference trees  ","href":"/research/","title":"Research"},{"content":"Selected Talks\n","href":"/talks/","title":"Talks"},{"content":"[2019]\nEuropean Meeting of Statisticians \u0026ndash; July, Palermo (IT)\nPersistent homology for kernel density exploration\n Networking IBS Regions \u0026ndash; July, Naples (IT) [best oral presentation award - slides]\nWasserstein Consensus for Bayesian Sample Size Determinations\n CRONOS meeting \u0026ndash; April, Limassol (CY) [talk]\nTopological low dimensional learning of high dimensional time series\n [2018]\nCMS-Statistics \u0026ndash; December, Pisa (IT) [talk]\nTopological Invariants for high-dimensional Time Series\n KAUST Statistics Workshop \u0026ndash; November, Thuwal (KSA) [poster]\nPredicting Dengue Epidemics in Brazil using Quantile Regression\n ISBA World Meeting \u0026ndash; June, Edinburgh (UK) [poster]\nBayesian Quantile Regression for Discrete Data\n Riunione Scientifica SIS \u0026ndash; June, Palermo (IT) [talk]\nIndefinite Topological Kernels\n Start-UP Research follow UP \u0026ndash; June, Palermo (IT) [talk]\nHierarchical Graphical Model for learning about functional network determinants\n Young Researcher Memotef \u0026ndash; April, Rome (IT) [talk]\nBayesian Quantile Regression for Discrete Data\n [2017]\nKAUST Statistics Workshop \u0026ndash; November, Thuwal (KSA) [poster]\nBayesian Quantile Regression for Discrete Data\n Riunione Intermedia SIS \u0026ndash; June, Firenze (IT) [best poster award]\nTopological summaries for Time-Varying Data\n IBS - Channel Section \u0026ndash; April, Hasselt (BE) [talk]\nTopological Data Analysis for Activity Recognition\n [2016]\nRiunione Scientifica SIS \u0026ndash; June, Fisciano (IT) [talk]\nTopological signatures for classification\n","href":"/talks/","title":"Talks"},{"content":"some dated lectures and tutoring material  Introduction to NIMBLE (July 2021)  pt I - Writing and running a NIMBLE model pt II - Making sense of a NIMBLE model pt III - More advanced stuff     Introduction to INLA (Dec 2019)    Statistics crash course (Sept 2016)\n Descriptive Statistics Probability Statistical Inference    ","href":"/teaching/","title":"Teaching"},{"content":"","href":"/docs/","title":"Docs"},{"content":"website is still a work in progress, things may not be where they are supposed to - yet!\n","href":"/docs/nimble_course/","title":"Work in Progress"},{"content":"website is still a work in progress, things may not be where they are supposed to - yet!\n","href":"/docs/wip/","title":"Work in Progress"},{"content":"","href":"/docs/mrc-talk/","title":"pflamelet is on CRAN"},{"content":"","href":"/docs/copyofcomments-support/","title":"Talk for the MRC Centre for Environment and Health"},{"content":"","href":"/docs/comments-support/","title":"COVID + Airpollution is in press"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/authors/muniftanjim/","title":"muniftanjim"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/lezioni/","title":"Lezionis"},{"content":"  Ingredienti di base Riprendiamo il classico setup visto ad inferenza, ossia supponiamo di avere un campione \\(X_1,\\dots X_n\\) di variabili indipendenti ed identicamente distribuite (i.i.d.) proveniente da una distribuzione \\(F\\), dove \\(F(x) = \\mathbb{P}(X_i\\leq x)\\).\nIndichiamo con \\(\\mu_F\\) e \\(\\sigma^2 _F\\) rispettivamente la media e la varianza della generica variabile \\(X_i\\). La notazione con \\(F\\) sottoscritto serve a ricordare che sia \\(\\mu_F\\) che \\(\\sigma^2_F\\) sono funzioni della funzione di ripartizione, infatti:\n \\(\\mu_F = \\mathbb{E}_F[X_i] = \\int xdF(x)\\) \\(\\sigma^2_F = \\mathbb{V}_F[X_i] = \\int (x-\\mu)^2 dF(x)\\)  Consideriamo poi una statistica \\(T_n =h(X_1,\\dots,X_n)\\).\nGoal(s) di oggi: quantificare l’incertezza su \\(T_n\\)\n stimandone la varianza \\(\\mathbb{V}_F [T_n]\\) costruendo un intervallo di confidenza per \\(T_n\\)   Stima della Varianza Attenzione: la varianza di una statistica \\(T_n\\) è sempre una funzione della funzione di ripartizione \\(F\\), che è incognita. Un esempio? Consideriamo la statistica più famosa di sempre, la media campionaria, ossia \\(T_n =\\bar{X}_n\\). Sempre da inferenza ricordiamo che \\[\\mathbb{V}[\\bar{X}_n]=\\frac{\\mathbb{V}[X]}{n} = \\frac{1}{n}\\int (x-\\mu)^2 dF(x).\\]\nIn generale possiamo scrivere la quantità di interesse \\(\\mathbb{V}_F [T_n]\\) come un integrale, e la settimana scorsa avete visto che, usando metodi Monte Carlo, si possono approssimare integrali molto complicati simulando.\nNel nostro esempio il problema sarebbe ridotto a:\nGenerare un nuovo campione \\(X_1^*,\\dots,X_n^* \\sim F\\) Calcolare \\(T_n^* =h(X_1^*,\\dots,X_n^*)\\) Ripetere il procedimento \\(B\\) volte in modo da ottenere \\(T_{n,1}^*,\\dots, T_{n,B}^*\\) osservazioni Approssimare \\(\\mathbb{V}_F [T_n]\\) con la varianza campionaria di \\(T_{n,1}^*,\\dots, T_{n,B}^*\\); per la legge dei grandi numeri si ha che al crescere di \\(B\\) \\[ vboot = \\frac{1}{B}\\sum_{i=1} ^B (T^*_{n,i} - \\bar{T}^*)^2 \\rightarrow \\mathbb{V}_F [T_n]. \\]  Purtroppo \\(F\\) è incognita e non possiamo usarla per generare nuove osservazioni, ma possiamo stimarla. L’idea alla base del bootstrap è semplice:\nStimare \\(F\\) incognita con una \\(\\hat{F}\\) nota (e di conseguenza stimare \\(\\mathbb{V}_F [T_n]\\) con \\(\\mathbb{V}_{\\hat{F}} [T_n]\\)). Approssimare \\(\\mathbb{V}_{\\hat{F}} [T_n]\\) simulando da \\(\\hat{F}\\).  Nel bootstrap ci sono dunque due tipi di errore:\n\\(\\hat{F}\\) non è \\(F\\) (e la differenza potrebbe non essere trascurabile) \\(vboot \\approx \\mathbb{V}_{\\hat{F}} [T_n]\\) (ma questo errore puè essere reso piccolo a piacere).  Bootstrap Empirico (o non-parametrico) Il modo più semplice di stimare la funzione di ripartizione \\(F\\) è attraverso la funzione di ripartizione empirica \\(\\mathbb{F}_n\\), definita come \\[\\mathbb{F}_n(x) = \\sum_{i=1}^n \\mathbb{I}[X_i\\leq x].\\]\nLa distribuzione empirica mette massa di probabilità \\(1/n\\) su ciascuna osservazione \\(X_i\\) quindi simulare da \\(\\mathbb{F}_n\\) non vuol dire altro estrarre con ripetizione dal campione “originale” \\(X_1,\\dots, X_n\\).\nESEMPIO:\nConsideriamo \\(X_1,\\dots, X_n \\sim N(0,1)\\).\nXn = rnorm(100) Per generare un campione \\(X_1^*,\\dots, X_n^* \\sim \\mathbb{F}_n\\) possiamo usare la funzione sample:\nXstar = sample(Xn, size=100, replace=TRUE) Confrontiamo ora la distribuzione empirica con quella teorica:\nhist(Xn, col=rgb(1,0,0,.3), probability = T, main=\u0026quot;\u0026quot;) hist(Xstar, add=TRUE, col=rgb(0,0,1,.3), probability = T) curve(dnorm, col=\u0026quot;red\u0026quot;, lwd=3, add=TRUE) La ragione per cui scegliamo \\(\\mathbb{F}_n\\) come stima di \\(F\\) è che al crescere della numerosità campionaria \\(n\\), le due distribuzioni sono equivalenti.\nXn = rnorm(10000) Xstar = sample(Xn, size=10000, replace=TRUE) hist(Xn, col=rgb(1,0,0,.3), probability = T) hist(Xstar, add=TRUE, col=rgb(0,0,1,.3), probability = T) curve(dnorm, col=\u0026quot;red\u0026quot;, lwd=3, add=TRUE) ESEMPIO: Bootstrap per la mediana Ora (finalmente!) vediamo come calcolare la stima bootstrap della varianza di una statistica, in particolare scegliamo \\(T_n = \\text{Mediana}(X_1,\\dots, X_n)\\).\nXn = rexp(100) Tn = median(Xn) boot.median=function(X, B=10000){ Tstar=c() for(i in 1:B ){ Xstar = sample(Xn, size=length(Xn), replace=TRUE) Tstar[i] = median(Xstar) } return(Tstar) } boot.emp = boot.median(Xn) L’approssimazione di \\(\\mathbb{V}_{\\hat{F}} [T_n]\\) è:\nvar(boot.emp) ## [1] 0.01049028 Il vettore Tstar contiene il campione \\(T_{n,1}^*,\\dots, T_{n,B}^*\\), possiamo analizzarne la distribuzione campionaria:\nhist(boot.emp, col=rgb(.0,.6,.3,.3), probability = T)   Bootstrap Parametrico Utilizzare la distribuzione empirica non è l’unico modo di stimare \\(F\\). Ad esempio se \\(F\\) è una distribuzione parametrica, \\(F=F_{\\theta}\\), possiamo usare il campione \\(X_1,\\dots,X_n\\) per ottenere una stima \\(\\hat{\\theta}\\) per il parametro \\(\\theta\\) e considerare lo stimatore plug-in \\(\\hat{F}=F_{\\hat{\\theta}}\\).\nConsideriamo lo stesso esempio di prima, i.e. un campione di \\(n=100\\) osservazioni generate da una v.a. esponenziale di parametro \\(\\lambda=1\\). Supponiamo di non conoscere il parametro e di doverlo stimare, ad esempio con il metodo della massima verosimiglianza:\nLL = function(lambda) { R = dexp(Xn, lambda) -sum(log(R)) } lambda = optim(par=1, LL)$par ## Warning in optim(par = 1, LL): one-dimensional optimization by Nelder-Mead is unreliable: ## use \u0026quot;Brent\u0026quot; or optimize() directly param.boot.median=function(X, thetahat, B=10000){ Tstar=c() for(i in 1:B ){ Xstar = rexp(length(X), rate=thetahat) Tstar[i] = median(Xstar) } return(Tstar) } boot.param = param.boot.median(Xn, thetahat = lambda) L’approssimazione di \\(\\mathbb{V}_{\\hat{F}} [T_n]\\) è:\nvar(boot.param) ## [1] 0.01117669 Il vettore Tstar contiene il campione \\(T_{n,1}^*,\\dots, T_{n,B}^*\\), possiamo analizzarne la distribuzione campionaria:\nhist(boot.emp, col=rgb(0,.6,0.3,.3), probability = T, main = \u0026quot;\u0026quot;) hist(boot.param, col=rgb(0,.1,.6,.3), probability = T, add=TRUE) Bootstrap Empirico vs Parametrico  Se il modello è specificato correttemente, il bootstrap parametrico è preferibile perchè \\(F_{\\hat{\\theta}}\\) converge ad \\(F\\) più velocemente di \\(\\mathbb{F}_n\\). Se l’assunzione distribuzionale è sbagliata però, \\(F_{\\hat{\\theta}}\\) non converge ad \\(F\\).     Intervalli di Confidenza Esistono diversi modi per costruire intervalli di confidenza utilizzando procedure bootstrap.\n1. Approssimazione Normale Se la statistica \\(T_n\\) ha distribuzione approssimativamente normale, possiamo utilizzare \\[T_n \\pm z_{1-\\alpha/2} \\sqrt{vboot} \\]\nConsideriamo l’esempio dell’esponenziale. In questo caso la distribuzione campionaria della statistica \\(T_n =\\text(Mediana)(X_1,\\dots,X_n)\\) può essere considerata normale:\nhist(boot.param, col=rgb(.1,.0,.6,.3), prob=TRUE) qqnorm(boot.param, pch=20, col=rgb(.8,0,.4,.4)) qqline(boot.param) L’intervallo di confidenza per \\(T_n\\) è quindi:\nL.int = Tn - 1.96 * sd(boot.param) U.int = Tn + 1.96 * sd(boot.param) N.int = c(L.int, U.int) N.int ## [1] 0.6207888 1.0352108  2. Percentili Abbiamo detto che \\(T_{n,1}^*,\\dots, T_{n,B}^*\\) è un campione, largo a piacere, dalla distribuzione campionaria di \\(T_n\\). Fino ad ora ci siamo limitati ad usarlo per stimare la varianza ma possiamo farci molto di più! Ad esempio possiamo usare questo campione per stimare i quantili della distribuzione di \\(T_n\\). Un modo immediato per costruire un intervallo di confidenza è dunque usando i quantili empirici di livello \\(\\alpha/2\\) e \\((1-\\alpha/2)\\), \\(T^*_{n,\\alpha/2}, T^*_{n,1-\\alpha/2}\\) come estremi.\nPerc.int = quantile(boot.param, c(0.025, 0.975)) Perc.int ## 2.5% 97.5% ## 0.5520038 0.9614811  3. Quantità pivotali Il metodo dei percentili è immediato ma potrebbe non essere molto accurato, un’alternativa lievemente più precisa è il metodo del pivot, che considera la quantità pivotale \\(T^*_n - T_n\\). L’intervallo che si ottiene con questo metodo è il seguente: \\[[2T_n - T^*_{n,1-\\alpha/2}, 2T_n -T^*_{n,\\alpha/2}] \\]\nPivot.int = c(2*Tn - quantile(boot.param, c(0.975, 0.025))) Pivot.int ## 97.5% 2.5% ## 0.6945184 1.1039958 EXTRA - “Studentizzazione”: Il metodo delle quantità pivotali standard si basa sull’assunzione che \\(T_n - T\\) e \\(T^*_n - T_n\\) abbiano approssimativamente la stessa distribuzione. Quando questo non è vero, si può ricorrere alla studentizzazione, ossia considerare: \\[\\tau = \\frac{T_n - T}{\\sqrt{\\mathbb{V}[T_n]}} \\] \\[\\tau^* = \\frac{T^*_n - T_n}{\\sqrt{\\mathbb{V}[T^*_n]}} \\]\nIl vantaggio delle quantità “studentizzate” \\(\\tau\\) e \\(\\tau^*\\) è che hanno approssimativamente la stessa distribuzione anche quando \\(T_n - T\\) e \\(T^*_n - T_n\\) hanno distribuzioni diverse. Lo svantaggio di questo metodo è che richiedono un secondo giro di bootstrap per stimare la quantitè \\(\\sqrt{\\mathbb{V}[T^*_n]}\\)\n  Errori \u0026amp; Subsampling Riprendiamo “Old Faithful”, i dati che avete visto nell’homework 1(?) relativi alla durata e ai tempi di attesa delle eruzioni di un vulcano.\ndata(\u0026quot;faithful\u0026quot;) Supponiamo di essere interessati a studiare la durata massima di un’eruzione, ossia \\(T_n=X_{(n)}=\\max\\{X_1,\\dots,X_n\\}\\):\nXn = faithful$eruptions Tn = max(Xn) Tn ## [1] 5.1 boot.max=function(X, B=10000){ Tstar=c() for(i in 1:B ){ Xstar = sample(X, size=length(X), replace=TRUE) Tstar[i] = max(Xstar) } int.boot = 2*max(X) - quantile(Tstar, c(0.975, 0.025)) return(list(Tstar=Tstar, int.boot=int.boot)) } boot.faith = boot.max(Xn) La distribuzione campionaria di \\(X_n\\) stimata con il bootstrap è un po’ “strana”.\nhist(boot.faith$Tstar, probability = TRUE, col=rgb(.9,0,0,.3), main=\u0026quot;\u0026quot;) Cerchiamo di capire se è un caso isolato o se c’è un problema di fondo. Siano \\(X_1,\\dots,X_n\\sim U(0, \\theta)\\), e sia \\(\\hat{\\theta}=X_{(n)}=max\\{X_1,\\dots,X_n\\}\\). Poniamo \\(\\theta=1\\) e generiamo un campione di \\(n=50\\) osservazioni:\nXn = runif(50) Tn = max(Xn) Tn ## [1] 0.9806278 boot.prova = boot.max(Xn) Ricordiamo che nel caso di \\(X_1,\\dots,X_n\\sim U(0, \\theta)\\) la distribuzione di \\(X_{(n)}\\) è \\[\\mathbb{P}(X_{(n)}\u0026lt;x)= \\left(\\frac{x}{\\theta}\\right)^n.\\] Confrontiamo la distribuzione stimata dal bootstrap con quella teorica (in blu).\nhist(boot.prova$Tstar, breaks=seq(0,1.3,by=0.05), prob= T, main=\u0026quot;\u0026quot;, col=rgb(.3,0.3,.7,.3)) curve(x^50, add=T, col=\u0026quot;darkblue\u0026quot;, lwd=4) Le due distribuzioni sembrano comportarsi in modo diverso. Vediamo quali sono gli effetti di questa differenza sulla stima intervallare: Costruiamo 1000 intervalli bootstrap e vediamo la proporzione di questi in cui è contenuto il vero valore del parametro \\(\\theta=1\\):\nboot.cis = replicate(1000, boot.max(X = runif(50), B = 1000)$int.boot) true.coverage = mean((1 \u0026gt;= boot.cis[1, ]) \u0026amp; (1 \u0026lt;= boot.cis[2, ])) true.coverage ## [1] 0.878 La copertura degli intervalli è decisamente inferiore a \\(0.95\\)! In generale il bootstrap si comporta “male” nelle stime degli estremi.\nUna possibile soluzione a questo problema è utilizzare un altro metodo di ricampionamento, il subsampling. In sostanza il subsampling è molto simile al bootstrap empirico, solo che al posto di ricampionare con ripetizione \\(n\\) elementi dai dati osservati, il campione di subsampling viene creato estraendo senza ripetizione \\(m\\) unitè dal campione originale (\\(m\\) deve essere piccolo).\nsub.max=function(X, B=10000, m){ Tstar=c() for(i in 1:B ){ Xstar = sample(X, size=m, replace=FALSE) Tstar[i] = max(Xstar) } int.boot = 2*max(X) - quantile(Tstar, c(0.975, 0.025)) return(list(Tstar=Tstar, int.sub=int.boot)) } Xn = runif(50) Tn = max(Xn) Tn ## [1] 0.9778377 sub.prova = sub.max(Xn, m=20) Analizziamo ora la copertura degli intervalli creati con il subsampling:\nsub.cis = replicate(1000, sub.max(X = runif(50), B = 1000, m=25)$int.sub) true.coverage = mean((1 \u0026gt;= sub.cis[1, ]) \u0026amp; (1 \u0026lt;= sub.cis[2, ])) true.coverage ## [1] 0.963  ","href":"/lezioni/2018-04-29-bootstrap/","title":"The almighty Bootstrap"},{"content":"  Ingredienti di base Riprendiamo il classico setup visto ad inferenza, ossia supponiamo di avere un campione \\(X_1,\\dots X_n\\) di variabili indipendenti ed identicamente distribuite (i.i.d.) proveniente da una distribuzione \\(F\\), dove \\(F(x) = \\mathbb{P}(X_i\\leq x)\\).\nIndichiamo con \\(\\mu_F\\) e \\(\\sigma^2 _F\\) rispettivamente la media e la varianza della generica variabile \\(X_i\\). La notazione con \\(F\\) sottoscritto serve a ricordare che sia \\(\\mu_F\\) che \\(\\sigma^2_F\\) sono funzioni della funzione di ripartizione, infatti:\n \\(\\mu_F = \\mathbb{E}_F[X_i] = \\int xdF(x)\\) \\(\\sigma^2_F = \\mathbb{V}_F[X_i] = \\int (x-\\mu)^2 dF(x)\\)  Consideriamo poi una statistica \\(T_n =h(X_1,\\dots,X_n)\\).\nGoal(s) di oggi: quantificare l’incertezza su \\(T_n\\)\n stimandone la varianza \\(\\mathbb{V}_F [T_n]\\) costruendo un intervallo di confidenza per \\(T_n\\)   Stima della Varianza Attenzione: la varianza di una statistica \\(T_n\\) è sempre una funzione della funzione di ripartizione \\(F\\), che è incognita. Un esempio? Consideriamo la statistica più famosa di sempre, la media campionaria, ossia \\(T_n =\\bar{X}_n\\). Sempre da inferenza ricordiamo che \\[\\mathbb{V}[\\bar{X}_n]=\\frac{\\mathbb{V}[X]}{n} = \\frac{1}{n}\\int (x-\\mu)^2 dF(x).\\]\nIn generale possiamo scrivere la quantità di interesse \\(\\mathbb{V}_F [T_n]\\) come un integrale, e la settimana scorsa avete visto che, usando metodi Monte Carlo, si possono approssimare integrali molto complicati simulando.\nNel nostro esempio il problema sarebbe ridotto a:\nGenerare un nuovo campione \\(X_1^*,\\dots,X_n^* \\sim F\\) Calcolare \\(T_n^* =h(X_1^*,\\dots,X_n^*)\\) Ripetere il procedimento \\(B\\) volte in modo da ottenere \\(T_{n,1}^*,\\dots, T_{n,B}^*\\) osservazioni Approssimare \\(\\mathbb{V}_F [T_n]\\) con la varianza campionaria di \\(T_{n,1}^*,\\dots, T_{n,B}^*\\); per la legge dei grandi numeri si ha che al crescere di \\(B\\) \\[ vboot = \\frac{1}{B}\\sum_{i=1} ^B (T^*_{n,i} - \\bar{T}^*)^2 \\rightarrow \\mathbb{V}_F [T_n]. \\]  Purtroppo \\(F\\) è incognita e non possiamo usarla per generare nuove osservazioni, ma possiamo stimarla. L’idea alla base del bootstrap è semplice:\nStimare \\(F\\) incognita con una \\(\\hat{F}\\) nota (e di conseguenza stimare \\(\\mathbb{V}_F [T_n]\\) con \\(\\mathbb{V}_{\\hat{F}} [T_n]\\)). Approssimare \\(\\mathbb{V}_{\\hat{F}} [T_n]\\) simulando da \\(\\hat{F}\\).  Nel bootstrap ci sono dunque due tipi di errore:\n\\(\\hat{F}\\) non è \\(F\\) (e la differenza potrebbe non essere trascurabile) \\(vboot \\approx \\mathbb{V}_{\\hat{F}} [T_n]\\) (ma questo errore puè essere reso piccolo a piacere).  Bootstrap Empirico (o non-parametrico) Il modo più semplice di stimare la funzione di ripartizione \\(F\\) è attraverso la funzione di ripartizione empirica \\(\\mathbb{F}_n\\), definita come \\[\\mathbb{F}_n(x) = \\sum_{i=1}^n \\mathbb{I}[X_i\\leq x].\\]\nLa distribuzione empirica mette massa di probabilità \\(1/n\\) su ciascuna osservazione \\(X_i\\) quindi simulare da \\(\\mathbb{F}_n\\) non vuol dire altro estrarre con ripetizione dal campione “originale” \\(X_1,\\dots, X_n\\).\nESEMPIO:\nConsideriamo \\(X_1,\\dots, X_n \\sim N(0,1)\\).\nXn = rnorm(100) Per generare un campione \\(X_1^*,\\dots, X_n^* \\sim \\mathbb{F}_n\\) possiamo usare la funzione sample:\nXstar = sample(Xn, size=100, replace=TRUE) Confrontiamo ora la distribuzione empirica con quella teorica:\nhist(Xn, col=rgb(1,0,0,.3), probability = T, main=\u0026quot;\u0026quot;) hist(Xstar, add=TRUE, col=rgb(0,0,1,.3), probability = T) curve(dnorm, col=\u0026quot;red\u0026quot;, lwd=3, add=TRUE) La ragione per cui scegliamo \\(\\mathbb{F}_n\\) come stima di \\(F\\) è che al crescere della numerosità campionaria \\(n\\), le due distribuzioni sono equivalenti.\nXn = rnorm(10000) Xstar = sample(Xn, size=10000, replace=TRUE) hist(Xn, col=rgb(1,0,0,.3), probability = T) hist(Xstar, add=TRUE, col=rgb(0,0,1,.3), probability = T) curve(dnorm, col=\u0026quot;red\u0026quot;, lwd=3, add=TRUE) ESEMPIO: Bootstrap per la mediana Ora (finalmente!) vediamo come calcolare la stima bootstrap della varianza di una statistica, in particolare scegliamo \\(T_n = \\text{Mediana}(X_1,\\dots, X_n)\\).\nXn = rexp(100) Tn = median(Xn) boot.median=function(X, B=10000){ Tstar=c() for(i in 1:B ){ Xstar = sample(Xn, size=length(Xn), replace=TRUE) Tstar[i] = median(Xstar) } return(Tstar) } boot.emp = boot.median(Xn) L’approssimazione di \\(\\mathbb{V}_{\\hat{F}} [T_n]\\) è:\nvar(boot.emp) ## [1] 0.01049028 Il vettore Tstar contiene il campione \\(T_{n,1}^*,\\dots, T_{n,B}^*\\), possiamo analizzarne la distribuzione campionaria:\nhist(boot.emp, col=rgb(.0,.6,.3,.3), probability = T)   Bootstrap Parametrico Utilizzare la distribuzione empirica non è l’unico modo di stimare \\(F\\). Ad esempio se \\(F\\) è una distribuzione parametrica, \\(F=F_{\\theta}\\), possiamo usare il campione \\(X_1,\\dots,X_n\\) per ottenere una stima \\(\\hat{\\theta}\\) per il parametro \\(\\theta\\) e considerare lo stimatore plug-in \\(\\hat{F}=F_{\\hat{\\theta}}\\).\nConsideriamo lo stesso esempio di prima, i.e. un campione di \\(n=100\\) osservazioni generate da una v.a. esponenziale di parametro \\(\\lambda=1\\). Supponiamo di non conoscere il parametro e di doverlo stimare, ad esempio con il metodo della massima verosimiglianza:\nLL = function(lambda) { R = dexp(Xn, lambda) -sum(log(R)) } lambda = optim(par=1, LL)$par ## Warning in optim(par = 1, LL): one-dimensional optimization by Nelder-Mead is unreliable: ## use \u0026quot;Brent\u0026quot; or optimize() directly param.boot.median=function(X, thetahat, B=10000){ Tstar=c() for(i in 1:B ){ Xstar = rexp(length(X), rate=thetahat) Tstar[i] = median(Xstar) } return(Tstar) } boot.param = param.boot.median(Xn, thetahat = lambda) L’approssimazione di \\(\\mathbb{V}_{\\hat{F}} [T_n]\\) è:\nvar(boot.param) ## [1] 0.01117669 Il vettore Tstar contiene il campione \\(T_{n,1}^*,\\dots, T_{n,B}^*\\), possiamo analizzarne la distribuzione campionaria:\nhist(boot.emp, col=rgb(0,.6,0.3,.3), probability = T, main = \u0026quot;\u0026quot;) hist(boot.param, col=rgb(0,.1,.6,.3), probability = T, add=TRUE) Bootstrap Empirico vs Parametrico  Se il modello è specificato correttemente, il bootstrap parametrico è preferibile perchè \\(F_{\\hat{\\theta}}\\) converge ad \\(F\\) più velocemente di \\(\\mathbb{F}_n\\). Se l’assunzione distribuzionale è sbagliata però, \\(F_{\\hat{\\theta}}\\) non converge ad \\(F\\).     Intervalli di Confidenza Esistono diversi modi per costruire intervalli di confidenza utilizzando procedure bootstrap.\n1. Approssimazione Normale Se la statistica \\(T_n\\) ha distribuzione approssimativamente normale, possiamo utilizzare \\[T_n \\pm z_{1-\\alpha/2} \\sqrt{vboot} \\]\nConsideriamo l’esempio dell’esponenziale. In questo caso la distribuzione campionaria della statistica \\(T_n =\\text(Mediana)(X_1,\\dots,X_n)\\) può essere considerata normale:\nhist(boot.param, col=rgb(.1,.0,.6,.3), prob=TRUE) qqnorm(boot.param, pch=20, col=rgb(.8,0,.4,.4)) qqline(boot.param) L’intervallo di confidenza per \\(T_n\\) è quindi:\nL.int = Tn - 1.96 * sd(boot.param) U.int = Tn + 1.96 * sd(boot.param) N.int = c(L.int, U.int) N.int ## [1] 0.6207888 1.0352108  2. Percentili Abbiamo detto che \\(T_{n,1}^*,\\dots, T_{n,B}^*\\) è un campione, largo a piacere, dalla distribuzione campionaria di \\(T_n\\). Fino ad ora ci siamo limitati ad usarlo per stimare la varianza ma possiamo farci molto di più! Ad esempio possiamo usare questo campione per stimare i quantili della distribuzione di \\(T_n\\). Un modo immediato per costruire un intervallo di confidenza è dunque usando i quantili empirici di livello \\(\\alpha/2\\) e \\((1-\\alpha/2)\\), \\(T^*_{n,\\alpha/2}, T^*_{n,1-\\alpha/2}\\) come estremi.\nPerc.int = quantile(boot.param, c(0.025, 0.975)) Perc.int ## 2.5% 97.5% ## 0.5520038 0.9614811  3. Quantità pivotali Il metodo dei percentili è immediato ma potrebbe non essere molto accurato, un’alternativa lievemente più precisa è il metodo del pivot, che considera la quantità pivotale \\(T^*_n - T_n\\). L’intervallo che si ottiene con questo metodo è il seguente: \\[[2T_n - T^*_{n,1-\\alpha/2}, 2T_n -T^*_{n,\\alpha/2}] \\]\nPivot.int = c(2*Tn - quantile(boot.param, c(0.975, 0.025))) Pivot.int ## 97.5% 2.5% ## 0.6945184 1.1039958 EXTRA - “Studentizzazione”: Il metodo delle quantità pivotali standard si basa sull’assunzione che \\(T_n - T\\) e \\(T^*_n - T_n\\) abbiano approssimativamente la stessa distribuzione. Quando questo non è vero, si può ricorrere alla studentizzazione, ossia considerare: \\[\\tau = \\frac{T_n - T}{\\sqrt{\\mathbb{V}[T_n]}} \\] \\[\\tau^* = \\frac{T^*_n - T_n}{\\sqrt{\\mathbb{V}[T^*_n]}} \\]\nIl vantaggio delle quantità “studentizzate” \\(\\tau\\) e \\(\\tau^*\\) è che hanno approssimativamente la stessa distribuzione anche quando \\(T_n - T\\) e \\(T^*_n - T_n\\) hanno distribuzioni diverse. Lo svantaggio di questo metodo è che richiedono un secondo giro di bootstrap per stimare la quantitè \\(\\sqrt{\\mathbb{V}[T^*_n]}\\)\n  Errori \u0026amp; Subsampling Riprendiamo “Old Faithful”, i dati che avete visto nell’homework 1(?) relativi alla durata e ai tempi di attesa delle eruzioni di un vulcano.\ndata(\u0026quot;faithful\u0026quot;) Supponiamo di essere interessati a studiare la durata massima di un’eruzione, ossia \\(T_n=X_{(n)}=\\max\\{X_1,\\dots,X_n\\}\\):\nXn = faithful$eruptions Tn = max(Xn) Tn ## [1] 5.1 boot.max=function(X, B=10000){ Tstar=c() for(i in 1:B ){ Xstar = sample(X, size=length(X), replace=TRUE) Tstar[i] = max(Xstar) } int.boot = 2*max(X) - quantile(Tstar, c(0.975, 0.025)) return(list(Tstar=Tstar, int.boot=int.boot)) } boot.faith = boot.max(Xn) La distribuzione campionaria di \\(X_n\\) stimata con il bootstrap è un po’ “strana”.\nhist(boot.faith$Tstar, probability = TRUE, col=rgb(.9,0,0,.3), main=\u0026quot;\u0026quot;) Cerchiamo di capire se è un caso isolato o se c’è un problema di fondo. Siano \\(X_1,\\dots,X_n\\sim U(0, \\theta)\\), e sia \\(\\hat{\\theta}=X_{(n)}=max\\{X_1,\\dots,X_n\\}\\). Poniamo \\(\\theta=1\\) e generiamo un campione di \\(n=50\\) osservazioni:\nXn = runif(50) Tn = max(Xn) Tn ## [1] 0.9806278 boot.prova = boot.max(Xn) Ricordiamo che nel caso di \\(X_1,\\dots,X_n\\sim U(0, \\theta)\\) la distribuzione di \\(X_{(n)}\\) è \\[\\mathbb{P}(X_{(n)}\u0026lt;x)= \\left(\\frac{x}{\\theta}\\right)^n.\\] Confrontiamo la distribuzione stimata dal bootstrap con quella teorica (in blu).\nhist(boot.prova$Tstar, breaks=seq(0,1.3,by=0.05), prob= T, main=\u0026quot;\u0026quot;, col=rgb(.3,0.3,.7,.3)) curve(x^50, add=T, col=\u0026quot;darkblue\u0026quot;, lwd=4) Le due distribuzioni sembrano comportarsi in modo diverso. Vediamo quali sono gli effetti di questa differenza sulla stima intervallare: Costruiamo 1000 intervalli bootstrap e vediamo la proporzione di questi in cui è contenuto il vero valore del parametro \\(\\theta=1\\):\nboot.cis = replicate(1000, boot.max(X = runif(50), B = 1000)$int.boot) true.coverage = mean((1 \u0026gt;= boot.cis[1, ]) \u0026amp; (1 \u0026lt;= boot.cis[2, ])) true.coverage ## [1] 0.878 La copertura degli intervalli è decisamente inferiore a \\(0.95\\)! In generale il bootstrap si comporta “male” nelle stime degli estremi.\nUna possibile soluzione a questo problema è utilizzare un altro metodo di ricampionamento, il subsampling. In sostanza il subsampling è molto simile al bootstrap empirico, solo che al posto di ricampionare con ripetizione \\(n\\) elementi dai dati osservati, il campione di subsampling viene creato estraendo senza ripetizione \\(m\\) unitè dal campione originale (\\(m\\) deve essere piccolo).\nsub.max=function(X, B=10000, m){ Tstar=c() for(i in 1:B ){ Xstar = sample(X, size=m, replace=FALSE) Tstar[i] = max(Xstar) } int.boot = 2*max(X) - quantile(Tstar, c(0.975, 0.025)) return(list(Tstar=Tstar, int.sub=int.boot)) } Xn = runif(50) Tn = max(Xn) Tn ## [1] 0.9778377 sub.prova = sub.max(Xn, m=20) Analizziamo ora la copertura degli intervalli creati con il subsampling:\nsub.cis = replicate(1000, sub.max(X = runif(50), B = 1000, m=25)$int.sub) true.coverage = mean((1 \u0026gt;= sub.cis[1, ]) \u0026amp; (1 \u0026lt;= sub.cis[2, ])) true.coverage ## [1] 0.963  ","href":"/the-almighty-bootstrap/","title":"The almighty Bootstrap"},{"content":" Introduzione a R Queste note costituiscono un’introduzione ad R, un linguaggio di programmazione rivolto all’analisi dei dati, utilizzato in statistica e data mining. Perchè proprio R?\n È gratuito: contrariamente ad altri software comunemente utilizzati in ambito economico-statistico (ad esempio SAS, STATA etc), R ? open-source. È aggiornato: quando viene presentata una nuova metodologia di analisi dei dati, molto spesso la prima implementazione ? proprio in R. È redditizio…  Informazioni introduttive possono essere trovate su wikipedia e r-project.org/about.html.\nUn comando in R è un’istruzione del tipo:\nprint(\u0026quot;hello, world\u0026quot;) che ha come risultato:\n## [1] \u0026quot;hello, world\u0026quot; Installazione di R Sul sito https://cran.r-project.org è possibile trovare i files da scaricare per l’installazione di R, oltre ad alcuni (forse anche troppo) dettagliati manuali.\nWindows Long Version Visitare il sito www.r-project.org e cliccare sul link CRAN che si trova sul menu a sinistra (sotto la voce “Download, Packages”). Da quest’ultima pagina è possibile selezionare un mirror da cui scaricare R. Sotto la voce Italy è possibile selezionare uno dei 4 mirror. Supponendo di aver scelto Milano, dovremmo trovarci alla pagina http://cran.mirror.garr.it/mirrors/CRAN/. Da qui, cliccare sul primo link: Download R for Windows e cliccare su base e quindi su Download R 3.3.1 for Windows. Dopo aver eseguito il file, R dovrebbe trovarsi in una cartella del tipo C:\\Programmi\\R\\R-3.3.1\\ (il percorso esatto dipende da dove si è scelto di installare il programma e dalla versione di R). Per avviarlo, cliccare su C:\\Programmi\\R\\R-3.3.1\\bin\\R.exe. Normalmente, durante l’installazione vengono messi un collegamento nel menu Start di Windows e/o un’icona sul desktop.\n Short Version Scaricare da questo link l’installer di R, eseguirlo ed avviare R dal collegamento sul desktop e/o dal menu Start.\n  Linux/BSD L’installazione di R su Linux dipende dalla distribuzione utilizzata:\n Ubuntu Debian RedHat   Mac Per installare R su un mac con sistema operativo successivo a Snowleopard è sufficiente eseguire il file scaricabile a questo link. Per versioni anteriori di OS X, informazioni aggiuntive e files da scaricare possono essere trovati qui.\nPer poter usufruire di tutte le funzionalita di R è necessario scaricare anche XQuartz che da OS X 10.9 (Mavericks) non è più parte del sistema operativo. Per l’utilizzo di R che faremo in questo laboratorio tuttavia questo non è richiesto.\n##Installazione RStudio\nL’interfaccia di R non è esattamente user-friendly, è perciò consigliabile utilizzare un IDE più intuitivo, come ad esempio RStudio. RStudio è solo un’interfaccia alternativa, è necessario installare R prima per poterlo usare. Informazioni aggiuntive su RStudio possono essere trovate su https://www.rstudio.com/. Per installare RStudio è sufficiente andare su questa pagina, scaricare la versione relativa al proprio sistema operativo (in particolare questa per Windows, questa per Mac,e questa per Linux ), eseguire il file così scaricato e seguire la procedura guidata di installazione.\n  Creare un progetto in RStudio Una delle funzioni più utili in RStudio è la creazione di progetti; questi permettono di raggruppare script, dati, history e workspaces (queste parole saranno più chiare alla fine della lezione!) in un’unica cartella. Per aprire un progetto basta andare su:\nFile \u0026gt; New project \u0026gt;…\nA questo punto vi viene chiesto se volete crearlo su una cartella (o directory) già esistente o su una nuova. Potete scegliere una directory già esistente selezionandola tramite il pulsante Browse.. e infine cliccate su Create project. A quel punto RStudio si posizionerà subito all’interno del nuovo progetto appena creato. Andando su:\nTools \u0026gt; Project Options \u0026gt;…\npotete modificare le impostazioni del vostro specifico progetto. Qualora esse siano fissate su Default si avrà come conseguenza che si comporteranno come le impostazioni globali (Tools \u0026gt; Global Options \u0026gt;…) di RStudio. Per uscire dal progetto basta andare su:\nFile \u0026gt; Close project\n  R basics Apertura, calcoli e chiusura Alla sua apertura, RStudio si presenta diviso in quattro sezioni:\n  Script: lo script è un file di testo in cui scrivere i comandi che vogliamo far eseguire ad R.\n Possiamo distinguere due sottofinestre:\n   Environment: mostra tutti gli oggetti presenti nello spazio di lavoro (anche detto workspace) History: contiene l’elenco di tutti i comandi eseguiti dalla console.  Console: qui vengono eseguiti i comandi di R e vengono mostrati i risultati. Nella console troverete il “prompt” presente sulla console e indicato (di default) da “\u0026gt;” che è il segnale di inizio per poter scrivere le istruzioni.\n Possiamo distinguere 5 sottofinestre:   Files: qui troverete tutti i file della working directory, cioè della cartella dove state lavorando, questi file sono gi stessi che trovereste nella cartella visualizzandola con esplora risorse Plots è la finestra dove visualizzerete tutti i grafici Packages: questa finestra vi mostra tutti i “pacchetti” installati e vi permette di installarne altri Help: la finestra “amica”, in essa infatti visualizzerete un aiuto (in inglese) per le istruzioni che userete Viewer: una finestra per funzioni avanzate che non utilizzeremo in questo corso.  Il motivo per cui utilizziamo R o un qualunque software statistico è delegare a lui calcoli che non sappiamo (o vogliamo) eseguire noi. Iniziamo a vedere cosa significa che R “esegue calcoli” partendo dal caso più semplice possibile, ossia le operazioni arimetiche:\n2+2 ## [1] 4 3*4 ## [1] 12 5*7-2 ## [1] 33 Esistono anche funzioni di R per operazioni più complesse come radice quadrata o logaritmo, ad esempio:\nsqrt(16) ## [1] 4 log(1) ## [1] 0 sqrt() e log() sono due funzioni. In R, le funzioni sono scritte come nome(argomento) dove nome è il nome della funzione e argomento (racchiuso tra parentesi tonde) è il valore in cui viene calcolata. Ad esempio, in sqrt(16) la funzione è sqrt() e l’argomento è 16, mentre in log(1) la funzione è log() e l’argomento 1.\nPer capire come usare una funzione possiamo servirci dei comandi help() e help.search() che servono ad aprire la documentazione di supporto. Usiamo la funzione help() quando conosciamo il nome di una funzione ma non sappiamo come lavori o cosa faccia di preciso, ad esempio help(sqrt) ci fornisce informazioni su come usare sqrt. Usiamo la funzione help.searc() quando sappiamo cosa vogliamo ottenere ma non come. Ad esempio supponiamo di voler calcolare il logaritmo di un numero ma di non conoscere la funzione log(), la funzione help.searc(“logarithm”) ci permette di trovare la funzione che ci serve. Osserviamo che quando utilizziamo help.search(), l’argomento della nostra ricerca deve essere indicato tra virgolette.\nhelp(sqrt) ??sqrt help.search(\u0026quot;logarithm\u0026quot;) Il risultato di un’operazione può essere salvato in R in modo tale da essere richiamato senza dover rieseguire tutti i calcoli. R permette infatti di assegnare l’output di un’operazione ad un oggetto, come illustrato di seguito:\nx = 2+2 x ## [1] 4 L’operazione appena effettuata viene chiamata assegnazione. Diciamo che il valore 2+2 è stato assegnato alla variabile x. Da questo momento in poi possiamo richiamare la variabile x se vogliamo evitare di ricalcolare 2+2. In questo esempio può sembrare quasi superfluo, ma le cose cambiano se cominciate a considerare quantità come\n(log(10)*2+sqrt(5))/4*cos(2*pi/3 + 1) In R ci sono più modi per effettuare un’assegnazione, in particolare i seguenti comandi sono equivalenti:\nx = 10 assign (\u0026quot;x\u0026quot;, 10) x \u0026lt;- 10 10 -\u0026gt; x Osserviamo che una volta creata tramite assegnazione, la variabile x è presente nel Global Environment, ed appare nel riquadro in alto a destra.\nNB: R è case sensitive, quindi fa distinzione tra x e X. R non considera invece lo spazio, quindi i comandi:\nx=10 x = 10 x = 10 sono per lui equivalenti. L’utilizzo di spazi può aiutare a rendere il codice più intellegibile; qui è possibile trovare alcune linee guida per scrivere codice comprensibile.\nEsistono tre principali tipi di variabili:\nnumerico (numeric) testuale (character) logico (logical)  var1 = 5 mode(var1)  ## [1] \u0026quot;numeric\u0026quot; La funzione mode ci permette di stabilire il tipo di una variabile. Ricordatevi che per sapere cosa fa una funzione si può usare il comando di help:\n?mode help(mode) Esempi di variabili character e logical sono invece:\nvar2 = \u0026quot;testo\u0026quot; mode(var2) ## [1] \u0026quot;character\u0026quot; var3 = TRUE mode(var3) ## [1] \u0026quot;logical\u0026quot; Le variabili logical possono assumere tre valori TRUE, FALSE e NA.\nNB: FALSE è logico mentre “FALSE” è testuale\n Vettori Definizione di vettore I vettori sono collezioni di elementi. Per creare un vettore usiamo la funzione c():\nvect1 = c(1,2,3) vect1 ## [1] 1 2 3 Un vettore è dunque costruito “concatenando” diversi elementi (c sta infatti per concatenate).\nvect2 = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;) vect2 ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; vect3 = c(TRUE, TRUE, FALSE) vect3 ## [1] TRUE TRUE FALSE Anche i vettori hanno dei tipi, in particolare è immediato vedere che:\n se tutti gli elementi sono numeric, il vettore è numeric:  mode(vect1)  ## [1] \u0026quot;numeric\u0026quot;  se tutti gli elementi sono character il vettore è character:  mode(vect2) ## [1] \u0026quot;character\u0026quot;  se tutti gli elementi sono logical il vettore è logical:  mode(vect3) ## [1] \u0026quot;logical\u0026quot; E se gli elementi fossero di tipi diversi? Un vettore è una collezione omogenea, ossia R assume che tutti i suoi elementi siano dello stesso tipo, anche se questo non è vero:\nvect4 = c(1,\u0026quot;stringa\u0026quot;, 4, FALSE) vect4 ## [1] \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; \u0026quot;FALSE\u0026quot; mode(vect4) ## [1] \u0026quot;character\u0026quot; Altri comandi per costruire vettori sono:\nvect5 = 1:10 # crea una sequenza da 1 a 10 vect5 ## [1] 1 2 3 4 5 6 7 8 9 10 vect6 = seq(from=1, to=10, by=0.5) # crea una sequenza da 1 a 10 con passo 0.5 vect6 ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 ## [15] 8.0 8.5 9.0 9.5 10.0 è possibile creare un vettore concatenando due o più vettori, ad esempio:\nvect7 = c(vect2, vect4) vect7 ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; ## [8] \u0026quot;FALSE\u0026quot;    Piccola Prova\n Costruire un vettore con il comando rep e commentare il tipo di oggetto ottenuto. Usate i comandi ? o help() per capire come funziona la funzione rep     Funzioni per vettori L’operatore che permette di selezionare un elemento all’interno di una collezione A di elementi è la parentesi quadra. Il comando A[ pos ] estrae da A l’elemento di posizione pos. Ad esempio, nel caso del vettore vect7 che ricordiamo essere\n## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; ## [8] \u0026quot;FALSE\u0026quot; il comando\nvect7[2] ## [1] \u0026quot;b\u0026quot; permette di selezionare il secondo elemento del vettore vect7.\nOsserviamo che è possibile selezionare più di un elemento alla volta, ad esempio:\nvect7[4:7]  ## [1] \u0026quot;d\u0026quot; \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; seleziona quarto, quinto, sesto e settimo elemento, mentre\nvect7[c(1,7)] ## [1] \u0026quot;a\u0026quot; \u0026quot;4\u0026quot; seleziona primo e settimo elemento.\nPossiamo usare anche condizioni logiche per selezionare elementi in un vettore, ad esempio:\nvect6[vect6\u0026gt;4] ## [1] 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 Funzioni utili per vettori sono:\n lenght(): conta il numero di elementi nel vettore  length(vect7)  ## [1] 8  sum(): somma gli elementi di un vettore (NB: vale solo per vettori numerici)  sum(vect6)  ## [1] 104.5  prod(): moltiplica gli elementi di un vettore (NB: vale solo per vettori numerici)  prod(vect6)  ## [1] 4.640392e+12  prodotto di un vettore per uno scalare  4*vect6  ## [1] 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40  somma di due vettori ( NB: vale solo per vettori numerici)  c(1,1,1) + c(2,3,4) ## [1] 3 4 5 R permette di sommare anche vettori di lunghezza diversa:\nvect5 + vect6 ## Warning in vect5 + vect6: longer object length is not a multiple of shorter ## object length ## [1] 2.0 3.5 5.0 6.5 8.0 9.5 11.0 12.5 14.0 15.5 7.0 8.5 10.0 11.5 ## [15] 13.0 14.5 16.0 17.5 19.0 ma è importante però notare che R produce un warning quando proviamo a farlo. Un warning vuol dire che R ha effettuato l’operazione ma in modo “creativo”. Più precisamente, per sommare i due vettori, R ha effettuato un riciclaggio, ossia ha riutilizzare gli elementi dell’oggetto la cui dimensione è inferiore in modo che la dimensione di tutti gli oggetti coinvolti sia la stessa.\n Un po’ di statistica Vediamo adesso alcune delle funzioni base dell’analisi statistica. Supponiamo di aver osservato un campione: . Funzioni utili:\n min(): calcola il valore minimo  min(campione) ## [1] 5.4  max(): calcola il valore massimo  max(campione) ## [1] 92.3  mean(): calcola la media  mean(campione) ## [1] 55.95  median(): calcola la mediana  median(campione)  ## [1] 62.5  range(): restituisce un vettore di due elementi, il min e max  range(campione) ## [1] 5.4 92.3  sd(): calcola la deviazione standard  sd(campione) ## [1] 29.23819  var(): calcola la varianza (corretta)  var(campione) ## [1] 854.8717    Piccola Prova\n Calcolare a mano utilizzando le funzioni sum e length la media, la varianza e la varianza corretta del campione.      Matrici Una matrice n k è una collezione di elementi composta da n righe e k colonne; intuitivamente può essere interpretato come n vettori di lunghezza k messi per riga o k vettori di lunghezza n affiancati per colonna. In R possiamo definire una matrice in diversi modi:\ndirettamente attraverso il comando matrix  mat1 = matrix(data=c(1:12), nrow=4, ncol=3, byrow =FALSE) mat1 ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 dove nrow e ncol rappresentano il numero di righe e colonne rispettivamente mat1.\nconcatenando righe con il comando rbind  mat2 = rbind(vect1, 5:7) mat2 ## [,1] [,2] [,3] ## vect1 1 2 3 ## 5 6 7 concatenando colonne con il comando cbind  mat3 = cbind(vect1, 5:7) mat3 ## vect1 ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 Per determinare le dimensioni di una matrice usiamo il comando dim():\ndim(mat1) ## [1] 4 3 dim(mat2) ## [1] 2 3 dim(mat3) ## [1] 3 2 Per selezionare un particolare elemento di una matrice usiamo ancora le parentesi quadre [ ], ma, mentre nel caso di un vettore c’era un solo indice di posizione (il posto dell’elemento nel vettore), nella matrice ce ne sono due, un indice di riga e uno di colonna. Per selezionare il generico elemento di riga i e colonna j dobbiamo specificare entrambi, separati da una virgola, ossia [riga, colonna]. Ad esempio\nmat1[1,2] ## [1] 5 seleziona l’elemento all’incrocio tra la prima riga e la seconda colonna.\nSe specifichiamo l’indice di riga ma non quello di colonna, ad esempio\nmat1[1,] R restituisce l’intera prima riga (che non ò altro che l’intersezione della prima riga con tutte le colonne). Analogamente, se specifichiamo solo l’indice di colonna, ad esempio 2, R ci restituirà l’intera seconda colonna\nmat1[1,] ## [1] 1 5 9 mat1[,2] ## [1] 5 6 7 8 NB: Anche se uno dei due indici è lasciato vuoto, le due posizioni devono sempre essere separate dalla virgola.\nOperazioni utili per le matrici sono:\n Trasposizione  t(mat1)  ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12  Somma tra matrici (NB: contrariamente al caso dei vettori, non è possibile sommare matrici di dimensioni diverse)  mat1+mat1  ## [,1] [,2] [,3] ## [1,] 2 10 18 ## [2,] 4 12 20 ## [3,] 6 14 22 ## [4,] 8 16 24  Moltiplicazione di una matrice per uno scalare  4*mat1 ## [,1] [,2] [,3] ## [1,] 4 20 36 ## [2,] 8 24 40 ## [3,] 12 28 44 ## [4,] 16 32 48  Determinante ()  det(mat1)   Calcolo dell’inversa  solve(mat1)  NB: Le ultime due operazioni producono un Error, questo vuol dire che R non ha eseguito l’istruzione che gli abbiamo fornito e ci informa di quale sia il motivo (in questo caso il problema è che non si possono calcolare determinante o inversa di una matrice non quadrata)\nnewmat = matrix(c(2,3,5.3,2,4,6,7,3,2), nrow=3, ncol=3) newmat ## [,1] [,2] [,3] ## [1,] 2.0 2 7 ## [2,] 3.0 4 3 ## [3,] 5.3 6 2 solve(newmat) ## [,1] [,2] [,3] ## [1,] 0.4424779 -1.6814159 0.97345133 ## [2,] -0.4380531 1.4646018 -0.66371681 ## [3,] 0.1415929 0.0619469 -0.08849558 det(newmat) ## [1] -22.6 Analogamente a quanto abbiamo visto per i vettori, anche le matrici sono collezioni omogenee, ossia R considera tutti gli elementi di una matrice dello stesso tipo.\nriga1 = c(0,2,4) riga2 = c(6,5,\u0026quot;a\u0026quot;) mat4 = rbind(riga1, riga2) mat4 ## [,1] [,2] [,3] ## riga1 \u0026quot;0\u0026quot; \u0026quot;2\u0026quot; \u0026quot;4\u0026quot; ## riga2 \u0026quot;6\u0026quot; \u0026quot;5\u0026quot; \u0026quot;a\u0026quot; mode(mat4) ## [1] \u0026quot;character\u0026quot;  Dataframes   Dataframes ————————————————————– I dataframes sono generalizzazioni delle matrici che permettono di considerare elementi di natura diversa.In sostanza un dataframe è una matrice in cui ogni colonna rappresenta una diversa variabile e come tale può avere tipo diverso.\nPer capire meglio questo concetto, analizziamo la costruzione di un dataframe:\nnome = c(\u0026quot;Marco\u0026quot;, \u0026quot;Giulio\u0026quot;, \u0026quot;Livia\u0026quot;, \u0026quot;Gaia\u0026quot;, \u0026quot;Carlo\u0026quot;) altezza = c(1.82, 1.77, 1.70, NA, 1.94) fuorisede = c(TRUE, FALSE, FALSE, FALSE, TRUE) fratelli = c(0, 0, 2, 1, 0) df = data.frame(nome, altezza, fuorisede, fratelli) df ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 2 Giulio 1.77 FALSE 0 ## 3 Livia 1.70 FALSE 2 ## 4 Gaia NA FALSE 1 ## 5 Carlo 1.94 TRUE 0 Il dataframe df contiene tutte le informazioni sul campione considerato; le sue righe rappresentano le unità campionarie (i vari intervistati), mentre le colonne rappresentano le variabili di interesse. È facile capire che il dataframe è l’oggetto principe nelle analisi statistiche con R. Il dataframe df è stato creato combinando oggetti di tipo diverso (nome è character, altezza è numeric, fuorisede è logical etc), che rimangono di tipo diverso. Per studiare la struttura di un dataframe usiamo il comando str()\nstr(df) ## \u0026#39;data.frame\u0026#39;: 5 obs. of 4 variables: ## $ nome : Factor w/ 5 levels \u0026quot;Carlo\u0026quot;,\u0026quot;Gaia\u0026quot;,..: 5 3 4 2 1 ## $ altezza : num 1.82 1.77 1.7 NA 1.94 ## $ fuorisede: logi TRUE FALSE FALSE FALSE TRUE ## $ fratelli : num 0 0 2 1 0 La prima cosa che ci dice è che l’oggetto df è un dataframe. In particolare questo dataframe è composto da 5 osservazioni di 4 variabili (nome,altezza,fuorisede e fratelli) che vediamo precedute da un segno di dollaro, $.\nIl dollaro viene utilizzato per estrarre una variabile da un dataframe, ad esempio se vogliamo solo l’altezza:\ndf$altezza ## [1] 1.82 1.77 1.70 NA 1.94 Ci sono altri modi per estrarre la stesse variabile dal dataframe, in particolare:\ndf[,2] # tutte le righe della seconda colonna, quella dell\u0026#39;altezza ## [1] 1.82 1.77 1.70 NA 1.94 df[2] # se è specificato un solo valore tra parentesi quadre nel caso dei dataframe è la colonna ## altezza ## 1 1.82 ## 2 1.77 ## 3 1.70 ## 4 NA ## 5 1.94 df[\u0026quot;altezza\u0026quot;] ## altezza ## 1 1.82 ## 2 1.77 ## 3 1.70 ## 4 NA ## 5 1.94 Naturalmente possiamo voler estrarre anche le righe (o unità) del dataframe:\ndf[1,] # estrae la prima riga ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 df[c(1,3),] # estrae la prima e la terza ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 3 Livia 1.70 FALSE 2 df[c(TRUE, TRUE, FALSE, FALSE, FALSE),] # estrae la prima e la seconda ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 2 Giulio 1.77 FALSE 0 Grazie ai dataframes è facile selezionare sottocampioni che soddisfino una data proprietà. Supponiamo ad esempio di voler studiare solo la sottopopolazione degli “alti” ossia tutti gli individui del campione di altezza superiore a 1.80, ossia tutti quelli per cui è vera la seguente condizione logica\ndf[\u0026quot;altezza\u0026quot;]\u0026gt;1.80 ## altezza ## [1,] TRUE ## [2,] FALSE ## [3,] FALSE ## [4,] NA ## [5,] TRUE Il comando df[“altezza”]\u0026gt;1.80 ci restituisce un vettore di TRUE, FALSE che possiamo utilizzare per estrarre solo alcune righe da df\ndf[df[\u0026quot;altezza\u0026quot;]\u0026gt;1.80,] ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## NA \u0026lt;NA\u0026gt; NA NA NA ## 5 Carlo 1.94 TRUE 0 Se invece fossimo interessati alla sottopopolazione dei figli unici, ossia tutti gli individui per cui è valida la condizione logica fratelli == 0 è sufficiente considerare\ndf[ df$fratelli == 0,] ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 2 Giulio 1.77 FALSE 0 ## 5 Carlo 1.94 TRUE 0 Osserviamo che per un dataframe valgono tutte le operazioni definite per una matrice.\ndf[1,2] ## [1] 1.82 dim(df) ## [1] 5 4 df+df ## Warning in Ops.factor(left, right): \u0026#39;+\u0026#39; not meaningful for factors ## nome altezza fuorisede fratelli ## 1 NA 3.64 2 0 ## 2 NA 3.54 0 0 ## 3 NA 3.40 0 4 ## 4 NA NA 0 2 ## 5 NA 3.88 2 0 Altre informazioni su di un dataframe:\nsummary(df) ## nome altezza fuorisede fratelli ## Carlo :1 Min. :1.700 Mode :logical Min. :0.0 ## Gaia :1 1st Qu.:1.752 FALSE:3 1st Qu.:0.0 ## Giulio:1 Median :1.795 TRUE :2 Median :0.0 ## Livia :1 Mean :1.808 Mean :0.6 ## Marco :1 3rd Qu.:1.850 3rd Qu.:1.0 ## Max. :1.940 Max. :2.0 ## NA\u0026#39;s :1 Liste Le liste sono una generalizzazione dei vettori al caso di elementi complessi. In sostanza una lista è un vettore dove ciascun elemento può essere un qualunque oggetto, ad esempio una matice, un vettore o anche un dataframe.\nUna lista può essere creata con il comando list():\nlista1 = list(a=mat1, b=df, c=vect1, d=var3) Per una lista valgono molti dei comandi già visti per dataframes, in particolare:\n str() ci permette di studiarne la struttura:  str(lista1) ## List of 4 ## $ a: int [1:4, 1:3] 1 2 3 4 5 6 7 8 9 10 ... ## $ b:\u0026#39;data.frame\u0026#39;: 5 obs. of 4 variables: ## ..$ nome : Factor w/ 5 levels \u0026quot;Carlo\u0026quot;,\u0026quot;Gaia\u0026quot;,..: 5 3 4 2 1 ## ..$ altezza : num [1:5] 1.82 1.77 1.7 NA 1.94 ## ..$ fuorisede: logi [1:5] TRUE FALSE FALSE FALSE TRUE ## ..$ fratelli : num [1:5] 0 0 2 1 0 ## $ c: num [1:3] 1 2 3 ## $ d: logi TRUE  $variabile ci permette di estrarre un elemento:  lista1$a ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12  Importare Dati In R i dati esterni vengono caricati in due modi:\n(versione facile) utilizzando il comando “import dataset” nel pannello environment in alto a destra.\n (versione difficile) utilizzando la funzione read.table() o read.csv().\n  Queste due funzioni sono assolutamente equivalenti (quindi useremo sempre la prima).\nProviamo ora ad importare il file Mc_data.csv, per rendere l’operazione più semplice, è comodo spostare il file che vogliamo caricare nella cartella dove stiamo lavorando (ossia quella del progetto).\nmc_data = read.csv(\u0026quot;Mc_data.csv\u0026quot;) Per capire che in che tipo di oggetto sono stati importati in R i dati possiamo:\n andare per tentativi, utilizzando la funzione is.  is.vector(mc_data) ## [1] FALSE is.matrix(mc_data) ## [1] FALSE is.data.frame(mc_data) ## [1] TRUE is.list(mc_data) ## [1] TRUE  utilizzare la funzione class:  class(mc_data) ## [1] \u0026quot;data.frame\u0026quot; mc_data è un dataframe, sappiamo dunque che informazioni utili possono essere estratte con:\nstr(mc_data) ## \u0026#39;data.frame\u0026#39;: 81 obs. of 17 variables: ## $ X : Factor w/ 81 levels \u0026quot;Angus Bacon \u0026amp; Cheese\u0026quot;,..: 1 2 3 11 59 21 29 12 28 20 ... ## $ Serving.Size : int 290 316 283 215 202 283 100 114 115 165 ... ## $ Calories : int 790 760 770 550 520 750 250 300 310 440 ... ## $ Total.Fat..g. : num 39 39 40 29 26 43 9 12 13 23 ... ## $ Saturated.Fat..g.: num 18 17 17 10 12 19 3.5 6 6 11 ... ## $ Trans.Fat..g. : num 2 2 2 1 1.5 2.5 0.5 0.5 0.5 1.5 ... ## $ Cholesterol..mg. : int 150 135 135 75 95 160 25 40 40 80 ... ## $ Sodium..mg. : int 1990 1640 1170 970 1100 1280 480 680 660 1050 ... ## $ Carbohydrates..g.: int 63 61 59 46 41 42 31 33 33 34 ... ## $ Dietary.Fiber..g.: int 4 4 4 3 3 3 2 2 2 2 ... ## $ Sugars..g. : int 13 10 8 9 10 10 6 7 7 7 ... ## $ Protein..g. : int 45 40 44 25 30 48 12 15 15 25 ... ## $ Vitamin.A...DV. : int 10 15 8 4 10 10 2 6 2 8 ... ## $ Vitamin.C...DV. : int 4 8 0 2 2 2 2 2 0 2 ... ## $ Calcium....DV. : int 30 30 40 25 30 30 10 20 20 30 ... ## $ Iron....DV. : int 35 35 35 25 25 35 15 15 15 20 ... ## $ Type : Factor w/ 6 levels \u0026quot;Breakfast\u0026quot;,\u0026quot;Burgers \u0026amp; Sandwiches\u0026quot;,..: 2 2 2 2 2 2 2 2 2 2 ... summary(mc_data) ## X Serving.Size ## Angus Bacon \u0026amp; Cheese : 1 Min. : 29.0 ## Angus Deluxe : 1 1st Qu.:117.0 ## Angus Mushroom \u0026amp; Swiss : 1 Median :164.0 ## Apple Slices : 1 Mean :177.6 ## Bacon, Egg \u0026amp; Cheese Bagel : 1 3rd Qu.:215.0 ## Bacon, Egg \u0026amp; Cheese Biscuit (Regular Size Biscuit): 1 Max. :420.0 ## (Other) :75 ## Calories Total.Fat..g. Saturated.Fat..g. Trans.Fat..g. ## Min. : 15.0 Min. : 0.00 Min. : 0.000 Min. :0.0000 ## 1st Qu.: 270.0 1st Qu.: 9.00 1st Qu.: 3.000 1st Qu.:0.0000 ## Median : 370.0 Median :16.00 Median : 5.000 Median :0.0000 ## Mean : 380.5 Mean :17.56 Mean : 6.537 Mean :0.2407 ## 3rd Qu.: 460.0 3rd Qu.:23.00 3rd Qu.: 9.000 3rd Qu.:0.0000 ## Max. :1090.0 Max. :56.00 Max. :19.000 Max. :2.5000 ## ## Cholesterol..mg. Sodium..mg. Carbohydrates..g. Dietary.Fiber..g. ## Min. : 0.00 Min. : 0.0 Min. : 4.00 Min. :0.000 ## 1st Qu.: 25.00 1st Qu.: 200.0 1st Qu.: 27.00 1st Qu.:1.000 ## Median : 45.00 Median : 700.0 Median : 34.00 Median :2.000 ## Mean : 76.67 Mean : 689.8 Mean : 39.06 Mean :2.259 ## 3rd Qu.: 70.00 3rd Qu.: 990.0 3rd Qu.: 51.00 3rd Qu.:3.000 ## Max. :575.00 Max. :2150.0 Max. :111.00 Max. :7.000 ## ## Sugars..g. Protein..g. Vitamin.A...DV. Vitamin.C...DV. ## Min. : 0.00 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 3.00 1st Qu.: 8.0 1st Qu.: 2.00 1st Qu.: 0.00 ## Median : 7.00 Median :15.0 Median : 6.00 Median : 2.00 ## Mean :13.33 Mean :16.7 Mean : 23.22 Mean : 11.73 ## 3rd Qu.:14.00 3rd Qu.:23.0 3rd Qu.: 10.00 3rd Qu.: 6.00 ## Max. :93.00 Max. :48.0 Max. :160.00 Max. :170.00 ## ## Calcium....DV. Iron....DV. Type ## Min. : 0.00 Min. : 0.00 Breakfast :19 ## 1st Qu.:10.00 1st Qu.: 6.00 Burgers \u0026amp; Sandwiches:22 ## Median :15.00 Median :10.00 Chicken : 4 ## Mean :16.27 Mean :12.86 Desserts/Shakes :16 ## 3rd Qu.:25.00 3rd Qu.:20.00 Salads :10 ## Max. :50.00 Max. :40.00 Snacks \u0026amp; Sides :10 ##  Selezioniamo solo i prodotti relativi alla colazione:\nmc_breakfast = mc_data[mc_data$Type==\u0026quot;Breakfast\u0026quot;,] Questo è ancora una volta un dataframe\nstr(mc_breakfast) ## \u0026#39;data.frame\u0026#39;: 19 obs. of 17 variables: ## $ X : Factor w/ 81 levels \u0026quot;Angus Bacon \u0026amp; Cheese\u0026quot;,..: 22 66 67 6 62 63 7 65 68 9 ... ## $ Serving.Size : int 138 114 164 140 117 163 164 141 201 269 ... ## $ Calories : int 300 370 450 420 430 510 410 420 550 740 ... ## $ Total.Fat..g. : num 12 22 27 23 27 33 18 22 31 48 ... ## $ Saturated.Fat..g.: num 5 8 10 12 12 14 8 8 12 17 ... ## $ Trans.Fat..g. : num 0 0 0 0 0 0 0 0 0 0 ... ## $ Cholesterol..mg. : int 260 45 285 240 30 250 240 35 265 555 ... ## $ Sodium..mg. : int 780 820 890 1130 1080 1170 1080 1030 1320 1560 ... ## $ Carbohydrates..g.: int 30 29 30 38 34 36 48 44 48 51 ... ## $ Dietary.Fiber..g.: int 2 2 2 2 2 2 2 2 2 3 ... ## $ Sugars..g. : int 3 2 2 3 2 2 15 15 15 3 ... ## $ Protein..g. : int 18 14 21 15 11 18 15 11 20 28 ... ## $ Vitamin.A...DV. : int 10 6 10 10 0 6 10 0 10 15 ... ## $ Vitamin.C...DV. : int 0 2 2 0 0 0 0 0 0 2 ... ## $ Calcium....DV. : int 30 25 30 15 6 10 20 8 20 15 ... ## $ Iron....DV. : int 15 15 20 15 15 20 15 10 15 25 ... ## $ Type : Factor w/ 6 levels \u0026quot;Breakfast\u0026quot;,\u0026quot;Burgers \u0026amp; Sandwiches\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(mc_breakfast) ## X Serving.Size ## Bacon, Egg \u0026amp; Cheese Bagel : 1 Min. : 56.0 ## Bacon, Egg \u0026amp; Cheese Biscuit (Regular Size Biscuit): 1 1st Qu.:127.5 ## Bacon, Egg \u0026amp; Cheese McGriddles : 1 Median :151.0 ## Big Breakfast (Regular Size Biscuit) : 1 Mean :170.3 ## Big Breakfast with Hotcakes (Regular Size Biscuit): 1 3rd Qu.:188.5 ## Cinnamon Melts : 1 Max. :420.0 ## (Other) :13 ## Calories Total.Fat..g. Saturated.Fat..g. Trans.Fat..g. ## Min. : 150 Min. : 4.50 Min. : 1.500 Min. :0.00000 ## 1st Qu.: 360 1st Qu.:17.00 1st Qu.: 7.000 1st Qu.:0.00000 ## Median : 420 Median :22.00 Median : 8.000 Median :0.00000 ## Mean : 460 Mean :23.55 Mean : 8.974 Mean :0.02632 ## 3rd Qu.: 515 3rd Qu.:27.00 3rd Qu.:12.000 3rd Qu.:0.00000 ## Max. :1090 Max. :56.00 Max. :19.000 Max. :0.50000 ## ## Cholesterol..mg. Sodium..mg. Carbohydrates..g. Dietary.Fiber..g. ## Min. : 0.0 Min. : 160.0 Min. : 15.00 Min. :1.000 ## 1st Qu.: 30.0 1st Qu.: 785.0 1st Qu.: 32.00 1st Qu.:2.000 ## Median :115.0 Median :1030.0 Median : 44.00 Median :2.000 ## Mean :172.6 Mean : 977.4 Mean : 46.37 Mean :2.579 ## 3rd Qu.:260.0 3rd Qu.:1175.0 3rd Qu.: 56.50 3rd Qu.:3.000 ## Max. :575.0 Max. :2150.0 Max. :111.00 Max. :6.000 ## ## Sugars..g. Protein..g. Vitamin.A...DV. Vitamin.C...DV. ## Min. : 0.000 Min. : 1.00 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 2.000 1st Qu.:11.00 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median :15.00 Median : 6.000 Median : 0.000 ## Mean : 9.632 Mean :15.53 Mean : 6.737 Mean : 7.684 ## 3rd Qu.:15.000 3rd Qu.:19.00 3rd Qu.:10.000 3rd Qu.: 2.000 ## Max. :32.000 Max. :36.00 Max. :20.000 Max. :130.000 ## ## Calcium....DV. Iron....DV. Type ## Min. : 0.00 Min. : 2.00 Breakfast :19 ## 1st Qu.: 9.00 1st Qu.:15.00 Burgers \u0026amp; Sandwiches: 0 ## Median :15.00 Median :15.00 Chicken : 0 ## Mean :15.32 Mean :16.42 Desserts/Shakes : 0 ## 3rd Qu.:20.00 3rd Qu.:17.50 Salads : 0 ## Max. :30.00 Max. :40.00 Snacks \u0026amp; Sides : 0 ##  Possiamo esportare questa selezione come file csv:\nwrite.csv(mc_breakfast, file = \u0026quot;mc_breakfast.csv\u0026quot;) Questo comando ha creato un file “mc_breakfast.csv” nella working directory. Osserviamo che questo è un file in formato .csv, per salvare gli stessi dati in un file .txt si può usare il comando write.table.\n Salva e Chiudi Prima di chiudere vogliamo salvare gli oggetti definiti nel laboratorio di oggi (o in generale in una sessione di R). Per sapere quali oggetti sono presenti nell’environment possiamo guardare il pannello in alto a dx o usare il comando ls():\nls() ## [1] \u0026quot;altezza\u0026quot; \u0026quot;campione\u0026quot; \u0026quot;df\u0026quot; \u0026quot;fratelli\u0026quot; ## [5] \u0026quot;fuorisede\u0026quot; \u0026quot;lista1\u0026quot; \u0026quot;mat1\u0026quot; \u0026quot;mat2\u0026quot; ## [9] \u0026quot;mat3\u0026quot; \u0026quot;mat4\u0026quot; \u0026quot;mc_breakfast\u0026quot; \u0026quot;mc_data\u0026quot; ## [13] \u0026quot;newmat\u0026quot; \u0026quot;nome\u0026quot; \u0026quot;riga1\u0026quot; \u0026quot;riga2\u0026quot; ## [17] \u0026quot;var1\u0026quot; \u0026quot;var2\u0026quot; \u0026quot;var3\u0026quot; \u0026quot;vect1\u0026quot; ## [21] \u0026quot;vect2\u0026quot; \u0026quot;vect3\u0026quot; \u0026quot;vect4\u0026quot; \u0026quot;vect5\u0026quot; ## [25] \u0026quot;vect6\u0026quot; \u0026quot;vect7\u0026quot; \u0026quot;x\u0026quot; Per sapere dove stiamo salvando, bisogna capire in che directory stiamo lavorando, questo può essere fatto con il comando getwd():\ngetwd() ## [1] \u0026quot;/Users/tulliapadellini/blog/blog/content/post_ita\u0026quot; In questo caso il comando ci restituisce la cartella che abbiamo scelto quando abbiamo creato il progetto, possiamo però decidere di lavorare in un altra directory, per farlo usiamo il comando setwd():\nsetwd(\u0026#39;/Users/tulliapadellini/Dropbox (Personal)/ESERCITAZIONI ROMA III/LAB01\u0026#39;) Per salvare gli oggetti presenti nell’environment globale, il workspace, possiamo usare il comando save.image(“nome_del_file.RData”).\nsave.image(\u0026quot;workspace_lab01.RData\u0026quot;) Volendo possiamo anche non salvare tutti gli oggetti, ad esempio se vogliamo salvare solo vect1 e mat1 usiamo il comando:\nsave(vect1, mat1, file=\u0026#39;oggetti.RData\u0026#39;) In sostanza save.image() corrisponde a save(list=ls()).\nAl contrario per eliminare un elemento dal workspace usiamo il comando rm()\nrm(mat1) Verifichiamo con ls() che l’oggetto mat1 non è più presente nel workspace:\nls() ## [1] \u0026quot;altezza\u0026quot; \u0026quot;campione\u0026quot; \u0026quot;df\u0026quot; \u0026quot;fratelli\u0026quot; ## [5] \u0026quot;fuorisede\u0026quot; \u0026quot;lista1\u0026quot; \u0026quot;mat2\u0026quot; \u0026quot;mat3\u0026quot; ## [9] \u0026quot;mat4\u0026quot; \u0026quot;mc_breakfast\u0026quot; \u0026quot;mc_data\u0026quot; \u0026quot;newmat\u0026quot; ## [13] \u0026quot;nome\u0026quot; \u0026quot;riga1\u0026quot; \u0026quot;riga2\u0026quot; \u0026quot;var1\u0026quot; ## [17] \u0026quot;var2\u0026quot; \u0026quot;var3\u0026quot; \u0026quot;vect1\u0026quot; \u0026quot;vect2\u0026quot; ## [21] \u0026quot;vect3\u0026quot; \u0026quot;vect4\u0026quot; \u0026quot;vect5\u0026quot; \u0026quot;vect6\u0026quot; ## [25] \u0026quot;vect7\u0026quot; \u0026quot;x\u0026quot; Se vogliamo rimuovere tutti oggetti dal workspace usiamo:\nrm(list=ls()) # USARE CON CAUTELA ls() ## character(0) Oltre allo script e al workspace possiamo salvare l’elenco dei comandi eseguiti, la “history” della sessione.\nsavehistory(\u0026quot;history_lab01.Rhistory\u0026quot;) Come abbiamo visto prima, la history si può trovare anche nel pannello in alto a destra della finestra di R Studio. Per caricare history e workspace usiamo load():\nload(\u0026quot;workspace_lab01.RData\u0026quot;) loadhistory(\u0026quot;history_lab01.Rhistory\u0026quot;)  Opzionale - Funzioni —————————————————- Per quanto R abbia moltissime funzioni già “pronte per l’uso” (abbiamo visto log, mean, sd, etc), non sempre è possibile trovarne una che risponda alle nostre esigenze; è utile dunque essere in grado di costruire a mano delle funzioni.\nIn R una funzione si costruisce con il comando function(). Una generica funzione si definisce come segue:\nnome_della_funzione = function( argomento della funzione ) espressione della funzione Iniziamo con un esempio molto semplice, la funzione quadrato:\nquadrato = function(x) x^2 In questo caso x è l’argomento della funzione, mentre quadrato è il suo nome. L’espressione della funzione è l’argomento elevato al quadrato (^2).\nSe vogliamo determinare il quadrato di 5, è sufficiente richiamare la funzione specificandone l’argomento:\nquadrato(5) ## [1] 25 Naturalmente si ha anche che:\nquadrato(3) ## [1] 9 et cetera.\nConsideriamo ora una funzione di due elementi, x e a :\npotenza = function(x,a) x^a In questo caso la funzione consiste nel prendere il primo argomento ed elevarlo ad una potenza pari al secondo:\npotenza(2, 2) ## [1] 4 potenza(3, 2) ## [1] 9 potenza(3, 3) ## [1] 27   ","href":"/lezioni/lab01/","title":"R - 101"},{"content":" Introduzione a R Queste note costituiscono un’introduzione ad R, un linguaggio di programmazione rivolto all’analisi dei dati, utilizzato in statistica e data mining. Perchè proprio R?\n È gratuito: contrariamente ad altri software comunemente utilizzati in ambito economico-statistico (ad esempio SAS, STATA etc), R ? open-source. È aggiornato: quando viene presentata una nuova metodologia di analisi dei dati, molto spesso la prima implementazione ? proprio in R. È redditizio…  Informazioni introduttive possono essere trovate su wikipedia e r-project.org/about.html.\nUn comando in R è un’istruzione del tipo:\nprint(\u0026quot;hello, world\u0026quot;) che ha come risultato:\n## [1] \u0026quot;hello, world\u0026quot; Installazione di R Sul sito https://cran.r-project.org è possibile trovare i files da scaricare per l’installazione di R, oltre ad alcuni (forse anche troppo) dettagliati manuali.\nWindows Long Version Visitare il sito www.r-project.org e cliccare sul link CRAN che si trova sul menu a sinistra (sotto la voce “Download, Packages”). Da quest’ultima pagina è possibile selezionare un mirror da cui scaricare R. Sotto la voce Italy è possibile selezionare uno dei 4 mirror. Supponendo di aver scelto Milano, dovremmo trovarci alla pagina http://cran.mirror.garr.it/mirrors/CRAN/. Da qui, cliccare sul primo link: Download R for Windows e cliccare su base e quindi su Download R 3.3.1 for Windows. Dopo aver eseguito il file, R dovrebbe trovarsi in una cartella del tipo C:\\Programmi\\R\\R-3.3.1\\ (il percorso esatto dipende da dove si è scelto di installare il programma e dalla versione di R). Per avviarlo, cliccare su C:\\Programmi\\R\\R-3.3.1\\bin\\R.exe. Normalmente, durante l’installazione vengono messi un collegamento nel menu Start di Windows e/o un’icona sul desktop.\n Short Version Scaricare da questo link l’installer di R, eseguirlo ed avviare R dal collegamento sul desktop e/o dal menu Start.\n  Linux/BSD L’installazione di R su Linux dipende dalla distribuzione utilizzata:\n Ubuntu Debian RedHat   Mac Per installare R su un mac con sistema operativo successivo a Snowleopard è sufficiente eseguire il file scaricabile a questo link. Per versioni anteriori di OS X, informazioni aggiuntive e files da scaricare possono essere trovati qui.\nPer poter usufruire di tutte le funzionalita di R è necessario scaricare anche XQuartz che da OS X 10.9 (Mavericks) non è più parte del sistema operativo. Per l’utilizzo di R che faremo in questo laboratorio tuttavia questo non è richiesto.\n##Installazione RStudio\nL’interfaccia di R non è esattamente user-friendly, è perciò consigliabile utilizzare un IDE più intuitivo, come ad esempio RStudio. RStudio è solo un’interfaccia alternativa, è necessario installare R prima per poterlo usare. Informazioni aggiuntive su RStudio possono essere trovate su https://www.rstudio.com/. Per installare RStudio è sufficiente andare su questa pagina, scaricare la versione relativa al proprio sistema operativo (in particolare questa per Windows, questa per Mac,e questa per Linux ), eseguire il file così scaricato e seguire la procedura guidata di installazione.\n  Creare un progetto in RStudio Una delle funzioni più utili in RStudio è la creazione di progetti; questi permettono di raggruppare script, dati, history e workspaces (queste parole saranno più chiare alla fine della lezione!) in un’unica cartella. Per aprire un progetto basta andare su:\nFile \u0026gt; New project \u0026gt;…\nA questo punto vi viene chiesto se volete crearlo su una cartella (o directory) già esistente o su una nuova. Potete scegliere una directory già esistente selezionandola tramite il pulsante Browse.. e infine cliccate su Create project. A quel punto RStudio si posizionerà subito all’interno del nuovo progetto appena creato. Andando su:\nTools \u0026gt; Project Options \u0026gt;…\npotete modificare le impostazioni del vostro specifico progetto. Qualora esse siano fissate su Default si avrà come conseguenza che si comporteranno come le impostazioni globali (Tools \u0026gt; Global Options \u0026gt;…) di RStudio. Per uscire dal progetto basta andare su:\nFile \u0026gt; Close project\n  R basics Apertura, calcoli e chiusura Alla sua apertura, RStudio si presenta diviso in quattro sezioni:\n  Script: lo script è un file di testo in cui scrivere i comandi che vogliamo far eseguire ad R.\n Possiamo distinguere due sottofinestre:\n   Environment: mostra tutti gli oggetti presenti nello spazio di lavoro (anche detto workspace) History: contiene l’elenco di tutti i comandi eseguiti dalla console.  Console: qui vengono eseguiti i comandi di R e vengono mostrati i risultati. Nella console troverete il “prompt” presente sulla console e indicato (di default) da “\u0026gt;” che è il segnale di inizio per poter scrivere le istruzioni.\n Possiamo distinguere 5 sottofinestre:   Files: qui troverete tutti i file della working directory, cioè della cartella dove state lavorando, questi file sono gi stessi che trovereste nella cartella visualizzandola con esplora risorse Plots è la finestra dove visualizzerete tutti i grafici Packages: questa finestra vi mostra tutti i “pacchetti” installati e vi permette di installarne altri Help: la finestra “amica”, in essa infatti visualizzerete un aiuto (in inglese) per le istruzioni che userete Viewer: una finestra per funzioni avanzate che non utilizzeremo in questo corso.  Il motivo per cui utilizziamo R o un qualunque software statistico è delegare a lui calcoli che non sappiamo (o vogliamo) eseguire noi. Iniziamo a vedere cosa significa che R “esegue calcoli” partendo dal caso più semplice possibile, ossia le operazioni arimetiche:\n2+2 ## [1] 4 3*4 ## [1] 12 5*7-2 ## [1] 33 Esistono anche funzioni di R per operazioni più complesse come radice quadrata o logaritmo, ad esempio:\nsqrt(16) ## [1] 4 log(1) ## [1] 0 sqrt() e log() sono due funzioni. In R, le funzioni sono scritte come nome(argomento) dove nome è il nome della funzione e argomento (racchiuso tra parentesi tonde) è il valore in cui viene calcolata. Ad esempio, in sqrt(16) la funzione è sqrt() e l’argomento è 16, mentre in log(1) la funzione è log() e l’argomento 1.\nPer capire come usare una funzione possiamo servirci dei comandi help() e help.search() che servono ad aprire la documentazione di supporto. Usiamo la funzione help() quando conosciamo il nome di una funzione ma non sappiamo come lavori o cosa faccia di preciso, ad esempio help(sqrt) ci fornisce informazioni su come usare sqrt. Usiamo la funzione help.searc() quando sappiamo cosa vogliamo ottenere ma non come. Ad esempio supponiamo di voler calcolare il logaritmo di un numero ma di non conoscere la funzione log(), la funzione help.searc(“logarithm”) ci permette di trovare la funzione che ci serve. Osserviamo che quando utilizziamo help.search(), l’argomento della nostra ricerca deve essere indicato tra virgolette.\nhelp(sqrt) ??sqrt help.search(\u0026quot;logarithm\u0026quot;) Il risultato di un’operazione può essere salvato in R in modo tale da essere richiamato senza dover rieseguire tutti i calcoli. R permette infatti di assegnare l’output di un’operazione ad un oggetto, come illustrato di seguito:\nx = 2+2 x ## [1] 4 L’operazione appena effettuata viene chiamata assegnazione. Diciamo che il valore 2+2 è stato assegnato alla variabile x. Da questo momento in poi possiamo richiamare la variabile x se vogliamo evitare di ricalcolare 2+2. In questo esempio può sembrare quasi superfluo, ma le cose cambiano se cominciate a considerare quantità come\n(log(10)*2+sqrt(5))/4*cos(2*pi/3 + 1) In R ci sono più modi per effettuare un’assegnazione, in particolare i seguenti comandi sono equivalenti:\nx = 10 assign (\u0026quot;x\u0026quot;, 10) x \u0026lt;- 10 10 -\u0026gt; x Osserviamo che una volta creata tramite assegnazione, la variabile x è presente nel Global Environment, ed appare nel riquadro in alto a destra.\nNB: R è case sensitive, quindi fa distinzione tra x e X. R non considera invece lo spazio, quindi i comandi:\nx=10 x = 10 x = 10 sono per lui equivalenti. L’utilizzo di spazi può aiutare a rendere il codice più intellegibile; qui è possibile trovare alcune linee guida per scrivere codice comprensibile.\nEsistono tre principali tipi di variabili:\nnumerico (numeric) testuale (character) logico (logical)  var1 = 5 mode(var1)  ## [1] \u0026quot;numeric\u0026quot; La funzione mode ci permette di stabilire il tipo di una variabile. Ricordatevi che per sapere cosa fa una funzione si può usare il comando di help:\n?mode help(mode) Esempi di variabili character e logical sono invece:\nvar2 = \u0026quot;testo\u0026quot; mode(var2) ## [1] \u0026quot;character\u0026quot; var3 = TRUE mode(var3) ## [1] \u0026quot;logical\u0026quot; Le variabili logical possono assumere tre valori TRUE, FALSE e NA.\nNB: FALSE è logico mentre “FALSE” è testuale\n Vettori Definizione di vettore I vettori sono collezioni di elementi. Per creare un vettore usiamo la funzione c():\nvect1 = c(1,2,3) vect1 ## [1] 1 2 3 Un vettore è dunque costruito “concatenando” diversi elementi (c sta infatti per concatenate).\nvect2 = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;) vect2 ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; vect3 = c(TRUE, TRUE, FALSE) vect3 ## [1] TRUE TRUE FALSE Anche i vettori hanno dei tipi, in particolare è immediato vedere che:\n se tutti gli elementi sono numeric, il vettore è numeric:  mode(vect1)  ## [1] \u0026quot;numeric\u0026quot;  se tutti gli elementi sono character il vettore è character:  mode(vect2) ## [1] \u0026quot;character\u0026quot;  se tutti gli elementi sono logical il vettore è logical:  mode(vect3) ## [1] \u0026quot;logical\u0026quot; E se gli elementi fossero di tipi diversi? Un vettore è una collezione omogenea, ossia R assume che tutti i suoi elementi siano dello stesso tipo, anche se questo non è vero:\nvect4 = c(1,\u0026quot;stringa\u0026quot;, 4, FALSE) vect4 ## [1] \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; \u0026quot;FALSE\u0026quot; mode(vect4) ## [1] \u0026quot;character\u0026quot; Altri comandi per costruire vettori sono:\nvect5 = 1:10 # crea una sequenza da 1 a 10 vect5 ## [1] 1 2 3 4 5 6 7 8 9 10 vect6 = seq(from=1, to=10, by=0.5) # crea una sequenza da 1 a 10 con passo 0.5 vect6 ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 ## [15] 8.0 8.5 9.0 9.5 10.0 è possibile creare un vettore concatenando due o più vettori, ad esempio:\nvect7 = c(vect2, vect4) vect7 ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; ## [8] \u0026quot;FALSE\u0026quot;    Piccola Prova\n Costruire un vettore con il comando rep e commentare il tipo di oggetto ottenuto. Usate i comandi ? o help() per capire come funziona la funzione rep     Funzioni per vettori L’operatore che permette di selezionare un elemento all’interno di una collezione A di elementi è la parentesi quadra. Il comando A[ pos ] estrae da A l’elemento di posizione pos. Ad esempio, nel caso del vettore vect7 che ricordiamo essere\n## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; ## [8] \u0026quot;FALSE\u0026quot; il comando\nvect7[2] ## [1] \u0026quot;b\u0026quot; permette di selezionare il secondo elemento del vettore vect7.\nOsserviamo che è possibile selezionare più di un elemento alla volta, ad esempio:\nvect7[4:7]  ## [1] \u0026quot;d\u0026quot; \u0026quot;1\u0026quot; \u0026quot;stringa\u0026quot; \u0026quot;4\u0026quot; seleziona quarto, quinto, sesto e settimo elemento, mentre\nvect7[c(1,7)] ## [1] \u0026quot;a\u0026quot; \u0026quot;4\u0026quot; seleziona primo e settimo elemento.\nPossiamo usare anche condizioni logiche per selezionare elementi in un vettore, ad esempio:\nvect6[vect6\u0026gt;4] ## [1] 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 Funzioni utili per vettori sono:\n lenght(): conta il numero di elementi nel vettore  length(vect7)  ## [1] 8  sum(): somma gli elementi di un vettore (NB: vale solo per vettori numerici)  sum(vect6)  ## [1] 104.5  prod(): moltiplica gli elementi di un vettore (NB: vale solo per vettori numerici)  prod(vect6)  ## [1] 4.640392e+12  prodotto di un vettore per uno scalare  4*vect6  ## [1] 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40  somma di due vettori ( NB: vale solo per vettori numerici)  c(1,1,1) + c(2,3,4) ## [1] 3 4 5 R permette di sommare anche vettori di lunghezza diversa:\nvect5 + vect6 ## Warning in vect5 + vect6: longer object length is not a multiple of shorter ## object length ## [1] 2.0 3.5 5.0 6.5 8.0 9.5 11.0 12.5 14.0 15.5 7.0 8.5 10.0 11.5 ## [15] 13.0 14.5 16.0 17.5 19.0 ma è importante però notare che R produce un warning quando proviamo a farlo. Un warning vuol dire che R ha effettuato l’operazione ma in modo “creativo”. Più precisamente, per sommare i due vettori, R ha effettuato un riciclaggio, ossia ha riutilizzare gli elementi dell’oggetto la cui dimensione è inferiore in modo che la dimensione di tutti gli oggetti coinvolti sia la stessa.\n Un po’ di statistica Vediamo adesso alcune delle funzioni base dell’analisi statistica. Supponiamo di aver osservato un campione: . Funzioni utili:\n min(): calcola il valore minimo  min(campione) ## [1] 5.4  max(): calcola il valore massimo  max(campione) ## [1] 92.3  mean(): calcola la media  mean(campione) ## [1] 55.95  median(): calcola la mediana  median(campione)  ## [1] 62.5  range(): restituisce un vettore di due elementi, il min e max  range(campione) ## [1] 5.4 92.3  sd(): calcola la deviazione standard  sd(campione) ## [1] 29.23819  var(): calcola la varianza (corretta)  var(campione) ## [1] 854.8717    Piccola Prova\n Calcolare a mano utilizzando le funzioni sum e length la media, la varianza e la varianza corretta del campione.      Matrici Una matrice n k è una collezione di elementi composta da n righe e k colonne; intuitivamente può essere interpretato come n vettori di lunghezza k messi per riga o k vettori di lunghezza n affiancati per colonna. In R possiamo definire una matrice in diversi modi:\ndirettamente attraverso il comando matrix  mat1 = matrix(data=c(1:12), nrow=4, ncol=3, byrow =FALSE) mat1 ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 dove nrow e ncol rappresentano il numero di righe e colonne rispettivamente mat1.\nconcatenando righe con il comando rbind  mat2 = rbind(vect1, 5:7) mat2 ## [,1] [,2] [,3] ## vect1 1 2 3 ## 5 6 7 concatenando colonne con il comando cbind  mat3 = cbind(vect1, 5:7) mat3 ## vect1 ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 Per determinare le dimensioni di una matrice usiamo il comando dim():\ndim(mat1) ## [1] 4 3 dim(mat2) ## [1] 2 3 dim(mat3) ## [1] 3 2 Per selezionare un particolare elemento di una matrice usiamo ancora le parentesi quadre [ ], ma, mentre nel caso di un vettore c’era un solo indice di posizione (il posto dell’elemento nel vettore), nella matrice ce ne sono due, un indice di riga e uno di colonna. Per selezionare il generico elemento di riga i e colonna j dobbiamo specificare entrambi, separati da una virgola, ossia [riga, colonna]. Ad esempio\nmat1[1,2] ## [1] 5 seleziona l’elemento all’incrocio tra la prima riga e la seconda colonna.\nSe specifichiamo l’indice di riga ma non quello di colonna, ad esempio\nmat1[1,] R restituisce l’intera prima riga (che non ò altro che l’intersezione della prima riga con tutte le colonne). Analogamente, se specifichiamo solo l’indice di colonna, ad esempio 2, R ci restituirà l’intera seconda colonna\nmat1[1,] ## [1] 1 5 9 mat1[,2] ## [1] 5 6 7 8 NB: Anche se uno dei due indici è lasciato vuoto, le due posizioni devono sempre essere separate dalla virgola.\nOperazioni utili per le matrici sono:\n Trasposizione  t(mat1)  ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12  Somma tra matrici (NB: contrariamente al caso dei vettori, non è possibile sommare matrici di dimensioni diverse)  mat1+mat1  ## [,1] [,2] [,3] ## [1,] 2 10 18 ## [2,] 4 12 20 ## [3,] 6 14 22 ## [4,] 8 16 24  Moltiplicazione di una matrice per uno scalare  4*mat1 ## [,1] [,2] [,3] ## [1,] 4 20 36 ## [2,] 8 24 40 ## [3,] 12 28 44 ## [4,] 16 32 48  Determinante ()  det(mat1)   Calcolo dell’inversa  solve(mat1)  NB: Le ultime due operazioni producono un Error, questo vuol dire che R non ha eseguito l’istruzione che gli abbiamo fornito e ci informa di quale sia il motivo (in questo caso il problema è che non si possono calcolare determinante o inversa di una matrice non quadrata)\nnewmat = matrix(c(2,3,5.3,2,4,6,7,3,2), nrow=3, ncol=3) newmat ## [,1] [,2] [,3] ## [1,] 2.0 2 7 ## [2,] 3.0 4 3 ## [3,] 5.3 6 2 solve(newmat) ## [,1] [,2] [,3] ## [1,] 0.4424779 -1.6814159 0.97345133 ## [2,] -0.4380531 1.4646018 -0.66371681 ## [3,] 0.1415929 0.0619469 -0.08849558 det(newmat) ## [1] -22.6 Analogamente a quanto abbiamo visto per i vettori, anche le matrici sono collezioni omogenee, ossia R considera tutti gli elementi di una matrice dello stesso tipo.\nriga1 = c(0,2,4) riga2 = c(6,5,\u0026quot;a\u0026quot;) mat4 = rbind(riga1, riga2) mat4 ## [,1] [,2] [,3] ## riga1 \u0026quot;0\u0026quot; \u0026quot;2\u0026quot; \u0026quot;4\u0026quot; ## riga2 \u0026quot;6\u0026quot; \u0026quot;5\u0026quot; \u0026quot;a\u0026quot; mode(mat4) ## [1] \u0026quot;character\u0026quot;  Dataframes   Dataframes ————————————————————– I dataframes sono generalizzazioni delle matrici che permettono di considerare elementi di natura diversa.In sostanza un dataframe è una matrice in cui ogni colonna rappresenta una diversa variabile e come tale può avere tipo diverso.\nPer capire meglio questo concetto, analizziamo la costruzione di un dataframe:\nnome = c(\u0026quot;Marco\u0026quot;, \u0026quot;Giulio\u0026quot;, \u0026quot;Livia\u0026quot;, \u0026quot;Gaia\u0026quot;, \u0026quot;Carlo\u0026quot;) altezza = c(1.82, 1.77, 1.70, NA, 1.94) fuorisede = c(TRUE, FALSE, FALSE, FALSE, TRUE) fratelli = c(0, 0, 2, 1, 0) df = data.frame(nome, altezza, fuorisede, fratelli) df ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 2 Giulio 1.77 FALSE 0 ## 3 Livia 1.70 FALSE 2 ## 4 Gaia NA FALSE 1 ## 5 Carlo 1.94 TRUE 0 Il dataframe df contiene tutte le informazioni sul campione considerato; le sue righe rappresentano le unità campionarie (i vari intervistati), mentre le colonne rappresentano le variabili di interesse. È facile capire che il dataframe è l’oggetto principe nelle analisi statistiche con R. Il dataframe df è stato creato combinando oggetti di tipo diverso (nome è character, altezza è numeric, fuorisede è logical etc), che rimangono di tipo diverso. Per studiare la struttura di un dataframe usiamo il comando str()\nstr(df) ## \u0026#39;data.frame\u0026#39;: 5 obs. of 4 variables: ## $ nome : Factor w/ 5 levels \u0026quot;Carlo\u0026quot;,\u0026quot;Gaia\u0026quot;,..: 5 3 4 2 1 ## $ altezza : num 1.82 1.77 1.7 NA 1.94 ## $ fuorisede: logi TRUE FALSE FALSE FALSE TRUE ## $ fratelli : num 0 0 2 1 0 La prima cosa che ci dice è che l’oggetto df è un dataframe. In particolare questo dataframe è composto da 5 osservazioni di 4 variabili (nome,altezza,fuorisede e fratelli) che vediamo precedute da un segno di dollaro, $.\nIl dollaro viene utilizzato per estrarre una variabile da un dataframe, ad esempio se vogliamo solo l’altezza:\ndf$altezza ## [1] 1.82 1.77 1.70 NA 1.94 Ci sono altri modi per estrarre la stesse variabile dal dataframe, in particolare:\ndf[,2] # tutte le righe della seconda colonna, quella dell\u0026#39;altezza ## [1] 1.82 1.77 1.70 NA 1.94 df[2] # se è specificato un solo valore tra parentesi quadre nel caso dei dataframe è la colonna ## altezza ## 1 1.82 ## 2 1.77 ## 3 1.70 ## 4 NA ## 5 1.94 df[\u0026quot;altezza\u0026quot;] ## altezza ## 1 1.82 ## 2 1.77 ## 3 1.70 ## 4 NA ## 5 1.94 Naturalmente possiamo voler estrarre anche le righe (o unità) del dataframe:\ndf[1,] # estrae la prima riga ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 df[c(1,3),] # estrae la prima e la terza ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 3 Livia 1.70 FALSE 2 df[c(TRUE, TRUE, FALSE, FALSE, FALSE),] # estrae la prima e la seconda ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 2 Giulio 1.77 FALSE 0 Grazie ai dataframes è facile selezionare sottocampioni che soddisfino una data proprietà. Supponiamo ad esempio di voler studiare solo la sottopopolazione degli “alti” ossia tutti gli individui del campione di altezza superiore a 1.80, ossia tutti quelli per cui è vera la seguente condizione logica\ndf[\u0026quot;altezza\u0026quot;]\u0026gt;1.80 ## altezza ## [1,] TRUE ## [2,] FALSE ## [3,] FALSE ## [4,] NA ## [5,] TRUE Il comando df[“altezza”]\u0026gt;1.80 ci restituisce un vettore di TRUE, FALSE che possiamo utilizzare per estrarre solo alcune righe da df\ndf[df[\u0026quot;altezza\u0026quot;]\u0026gt;1.80,] ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## NA \u0026lt;NA\u0026gt; NA NA NA ## 5 Carlo 1.94 TRUE 0 Se invece fossimo interessati alla sottopopolazione dei figli unici, ossia tutti gli individui per cui è valida la condizione logica fratelli == 0 è sufficiente considerare\ndf[ df$fratelli == 0,] ## nome altezza fuorisede fratelli ## 1 Marco 1.82 TRUE 0 ## 2 Giulio 1.77 FALSE 0 ## 5 Carlo 1.94 TRUE 0 Osserviamo che per un dataframe valgono tutte le operazioni definite per una matrice.\ndf[1,2] ## [1] 1.82 dim(df) ## [1] 5 4 df+df ## Warning in Ops.factor(left, right): \u0026#39;+\u0026#39; not meaningful for factors ## nome altezza fuorisede fratelli ## 1 NA 3.64 2 0 ## 2 NA 3.54 0 0 ## 3 NA 3.40 0 4 ## 4 NA NA 0 2 ## 5 NA 3.88 2 0 Altre informazioni su di un dataframe:\nsummary(df) ## nome altezza fuorisede fratelli ## Carlo :1 Min. :1.700 Mode :logical Min. :0.0 ## Gaia :1 1st Qu.:1.752 FALSE:3 1st Qu.:0.0 ## Giulio:1 Median :1.795 TRUE :2 Median :0.0 ## Livia :1 Mean :1.808 Mean :0.6 ## Marco :1 3rd Qu.:1.850 3rd Qu.:1.0 ## Max. :1.940 Max. :2.0 ## NA\u0026#39;s :1 Liste Le liste sono una generalizzazione dei vettori al caso di elementi complessi. In sostanza una lista è un vettore dove ciascun elemento può essere un qualunque oggetto, ad esempio una matice, un vettore o anche un dataframe.\nUna lista può essere creata con il comando list():\nlista1 = list(a=mat1, b=df, c=vect1, d=var3) Per una lista valgono molti dei comandi già visti per dataframes, in particolare:\n str() ci permette di studiarne la struttura:  str(lista1) ## List of 4 ## $ a: int [1:4, 1:3] 1 2 3 4 5 6 7 8 9 10 ... ## $ b:\u0026#39;data.frame\u0026#39;: 5 obs. of 4 variables: ## ..$ nome : Factor w/ 5 levels \u0026quot;Carlo\u0026quot;,\u0026quot;Gaia\u0026quot;,..: 5 3 4 2 1 ## ..$ altezza : num [1:5] 1.82 1.77 1.7 NA 1.94 ## ..$ fuorisede: logi [1:5] TRUE FALSE FALSE FALSE TRUE ## ..$ fratelli : num [1:5] 0 0 2 1 0 ## $ c: num [1:3] 1 2 3 ## $ d: logi TRUE  $variabile ci permette di estrarre un elemento:  lista1$a ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12  Importare Dati In R i dati esterni vengono caricati in due modi:\n(versione facile) utilizzando il comando “import dataset” nel pannello environment in alto a destra.\n (versione difficile) utilizzando la funzione read.table() o read.csv().\n  Queste due funzioni sono assolutamente equivalenti (quindi useremo sempre la prima).\nProviamo ora ad importare il file Mc_data.csv, per rendere l’operazione più semplice, è comodo spostare il file che vogliamo caricare nella cartella dove stiamo lavorando (ossia quella del progetto).\nmc_data = read.csv(\u0026quot;Mc_data.csv\u0026quot;) Per capire che in che tipo di oggetto sono stati importati in R i dati possiamo:\n andare per tentativi, utilizzando la funzione is.  is.vector(mc_data) ## [1] FALSE is.matrix(mc_data) ## [1] FALSE is.data.frame(mc_data) ## [1] TRUE is.list(mc_data) ## [1] TRUE  utilizzare la funzione class:  class(mc_data) ## [1] \u0026quot;data.frame\u0026quot; mc_data è un dataframe, sappiamo dunque che informazioni utili possono essere estratte con:\nstr(mc_data) ## \u0026#39;data.frame\u0026#39;: 81 obs. of 17 variables: ## $ X : Factor w/ 81 levels \u0026quot;Angus Bacon \u0026amp; Cheese\u0026quot;,..: 1 2 3 11 59 21 29 12 28 20 ... ## $ Serving.Size : int 290 316 283 215 202 283 100 114 115 165 ... ## $ Calories : int 790 760 770 550 520 750 250 300 310 440 ... ## $ Total.Fat..g. : num 39 39 40 29 26 43 9 12 13 23 ... ## $ Saturated.Fat..g.: num 18 17 17 10 12 19 3.5 6 6 11 ... ## $ Trans.Fat..g. : num 2 2 2 1 1.5 2.5 0.5 0.5 0.5 1.5 ... ## $ Cholesterol..mg. : int 150 135 135 75 95 160 25 40 40 80 ... ## $ Sodium..mg. : int 1990 1640 1170 970 1100 1280 480 680 660 1050 ... ## $ Carbohydrates..g.: int 63 61 59 46 41 42 31 33 33 34 ... ## $ Dietary.Fiber..g.: int 4 4 4 3 3 3 2 2 2 2 ... ## $ Sugars..g. : int 13 10 8 9 10 10 6 7 7 7 ... ## $ Protein..g. : int 45 40 44 25 30 48 12 15 15 25 ... ## $ Vitamin.A...DV. : int 10 15 8 4 10 10 2 6 2 8 ... ## $ Vitamin.C...DV. : int 4 8 0 2 2 2 2 2 0 2 ... ## $ Calcium....DV. : int 30 30 40 25 30 30 10 20 20 30 ... ## $ Iron....DV. : int 35 35 35 25 25 35 15 15 15 20 ... ## $ Type : Factor w/ 6 levels \u0026quot;Breakfast\u0026quot;,\u0026quot;Burgers \u0026amp; Sandwiches\u0026quot;,..: 2 2 2 2 2 2 2 2 2 2 ... summary(mc_data) ## X Serving.Size ## Angus Bacon \u0026amp; Cheese : 1 Min. : 29.0 ## Angus Deluxe : 1 1st Qu.:117.0 ## Angus Mushroom \u0026amp; Swiss : 1 Median :164.0 ## Apple Slices : 1 Mean :177.6 ## Bacon, Egg \u0026amp; Cheese Bagel : 1 3rd Qu.:215.0 ## Bacon, Egg \u0026amp; Cheese Biscuit (Regular Size Biscuit): 1 Max. :420.0 ## (Other) :75 ## Calories Total.Fat..g. Saturated.Fat..g. Trans.Fat..g. ## Min. : 15.0 Min. : 0.00 Min. : 0.000 Min. :0.0000 ## 1st Qu.: 270.0 1st Qu.: 9.00 1st Qu.: 3.000 1st Qu.:0.0000 ## Median : 370.0 Median :16.00 Median : 5.000 Median :0.0000 ## Mean : 380.5 Mean :17.56 Mean : 6.537 Mean :0.2407 ## 3rd Qu.: 460.0 3rd Qu.:23.00 3rd Qu.: 9.000 3rd Qu.:0.0000 ## Max. :1090.0 Max. :56.00 Max. :19.000 Max. :2.5000 ## ## Cholesterol..mg. Sodium..mg. Carbohydrates..g. Dietary.Fiber..g. ## Min. : 0.00 Min. : 0.0 Min. : 4.00 Min. :0.000 ## 1st Qu.: 25.00 1st Qu.: 200.0 1st Qu.: 27.00 1st Qu.:1.000 ## Median : 45.00 Median : 700.0 Median : 34.00 Median :2.000 ## Mean : 76.67 Mean : 689.8 Mean : 39.06 Mean :2.259 ## 3rd Qu.: 70.00 3rd Qu.: 990.0 3rd Qu.: 51.00 3rd Qu.:3.000 ## Max. :575.00 Max. :2150.0 Max. :111.00 Max. :7.000 ## ## Sugars..g. Protein..g. Vitamin.A...DV. Vitamin.C...DV. ## Min. : 0.00 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 3.00 1st Qu.: 8.0 1st Qu.: 2.00 1st Qu.: 0.00 ## Median : 7.00 Median :15.0 Median : 6.00 Median : 2.00 ## Mean :13.33 Mean :16.7 Mean : 23.22 Mean : 11.73 ## 3rd Qu.:14.00 3rd Qu.:23.0 3rd Qu.: 10.00 3rd Qu.: 6.00 ## Max. :93.00 Max. :48.0 Max. :160.00 Max. :170.00 ## ## Calcium....DV. Iron....DV. Type ## Min. : 0.00 Min. : 0.00 Breakfast :19 ## 1st Qu.:10.00 1st Qu.: 6.00 Burgers \u0026amp; Sandwiches:22 ## Median :15.00 Median :10.00 Chicken : 4 ## Mean :16.27 Mean :12.86 Desserts/Shakes :16 ## 3rd Qu.:25.00 3rd Qu.:20.00 Salads :10 ## Max. :50.00 Max. :40.00 Snacks \u0026amp; Sides :10 ##  Selezioniamo solo i prodotti relativi alla colazione:\nmc_breakfast = mc_data[mc_data$Type==\u0026quot;Breakfast\u0026quot;,] Questo è ancora una volta un dataframe\nstr(mc_breakfast) ## \u0026#39;data.frame\u0026#39;: 19 obs. of 17 variables: ## $ X : Factor w/ 81 levels \u0026quot;Angus Bacon \u0026amp; Cheese\u0026quot;,..: 22 66 67 6 62 63 7 65 68 9 ... ## $ Serving.Size : int 138 114 164 140 117 163 164 141 201 269 ... ## $ Calories : int 300 370 450 420 430 510 410 420 550 740 ... ## $ Total.Fat..g. : num 12 22 27 23 27 33 18 22 31 48 ... ## $ Saturated.Fat..g.: num 5 8 10 12 12 14 8 8 12 17 ... ## $ Trans.Fat..g. : num 0 0 0 0 0 0 0 0 0 0 ... ## $ Cholesterol..mg. : int 260 45 285 240 30 250 240 35 265 555 ... ## $ Sodium..mg. : int 780 820 890 1130 1080 1170 1080 1030 1320 1560 ... ## $ Carbohydrates..g.: int 30 29 30 38 34 36 48 44 48 51 ... ## $ Dietary.Fiber..g.: int 2 2 2 2 2 2 2 2 2 3 ... ## $ Sugars..g. : int 3 2 2 3 2 2 15 15 15 3 ... ## $ Protein..g. : int 18 14 21 15 11 18 15 11 20 28 ... ## $ Vitamin.A...DV. : int 10 6 10 10 0 6 10 0 10 15 ... ## $ Vitamin.C...DV. : int 0 2 2 0 0 0 0 0 0 2 ... ## $ Calcium....DV. : int 30 25 30 15 6 10 20 8 20 15 ... ## $ Iron....DV. : int 15 15 20 15 15 20 15 10 15 25 ... ## $ Type : Factor w/ 6 levels \u0026quot;Breakfast\u0026quot;,\u0026quot;Burgers \u0026amp; Sandwiches\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(mc_breakfast) ## X Serving.Size ## Bacon, Egg \u0026amp; Cheese Bagel : 1 Min. : 56.0 ## Bacon, Egg \u0026amp; Cheese Biscuit (Regular Size Biscuit): 1 1st Qu.:127.5 ## Bacon, Egg \u0026amp; Cheese McGriddles : 1 Median :151.0 ## Big Breakfast (Regular Size Biscuit) : 1 Mean :170.3 ## Big Breakfast with Hotcakes (Regular Size Biscuit): 1 3rd Qu.:188.5 ## Cinnamon Melts : 1 Max. :420.0 ## (Other) :13 ## Calories Total.Fat..g. Saturated.Fat..g. Trans.Fat..g. ## Min. : 150 Min. : 4.50 Min. : 1.500 Min. :0.00000 ## 1st Qu.: 360 1st Qu.:17.00 1st Qu.: 7.000 1st Qu.:0.00000 ## Median : 420 Median :22.00 Median : 8.000 Median :0.00000 ## Mean : 460 Mean :23.55 Mean : 8.974 Mean :0.02632 ## 3rd Qu.: 515 3rd Qu.:27.00 3rd Qu.:12.000 3rd Qu.:0.00000 ## Max. :1090 Max. :56.00 Max. :19.000 Max. :0.50000 ## ## Cholesterol..mg. Sodium..mg. Carbohydrates..g. Dietary.Fiber..g. ## Min. : 0.0 Min. : 160.0 Min. : 15.00 Min. :1.000 ## 1st Qu.: 30.0 1st Qu.: 785.0 1st Qu.: 32.00 1st Qu.:2.000 ## Median :115.0 Median :1030.0 Median : 44.00 Median :2.000 ## Mean :172.6 Mean : 977.4 Mean : 46.37 Mean :2.579 ## 3rd Qu.:260.0 3rd Qu.:1175.0 3rd Qu.: 56.50 3rd Qu.:3.000 ## Max. :575.0 Max. :2150.0 Max. :111.00 Max. :6.000 ## ## Sugars..g. Protein..g. Vitamin.A...DV. Vitamin.C...DV. ## Min. : 0.000 Min. : 1.00 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 2.000 1st Qu.:11.00 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median :15.00 Median : 6.000 Median : 0.000 ## Mean : 9.632 Mean :15.53 Mean : 6.737 Mean : 7.684 ## 3rd Qu.:15.000 3rd Qu.:19.00 3rd Qu.:10.000 3rd Qu.: 2.000 ## Max. :32.000 Max. :36.00 Max. :20.000 Max. :130.000 ## ## Calcium....DV. Iron....DV. Type ## Min. : 0.00 Min. : 2.00 Breakfast :19 ## 1st Qu.: 9.00 1st Qu.:15.00 Burgers \u0026amp; Sandwiches: 0 ## Median :15.00 Median :15.00 Chicken : 0 ## Mean :15.32 Mean :16.42 Desserts/Shakes : 0 ## 3rd Qu.:20.00 3rd Qu.:17.50 Salads : 0 ## Max. :30.00 Max. :40.00 Snacks \u0026amp; Sides : 0 ##  Possiamo esportare questa selezione come file csv:\nwrite.csv(mc_breakfast, file = \u0026quot;mc_breakfast.csv\u0026quot;) Questo comando ha creato un file “mc_breakfast.csv” nella working directory. Osserviamo che questo è un file in formato .csv, per salvare gli stessi dati in un file .txt si può usare il comando write.table.\n Salva e Chiudi Prima di chiudere vogliamo salvare gli oggetti definiti nel laboratorio di oggi (o in generale in una sessione di R). Per sapere quali oggetti sono presenti nell’environment possiamo guardare il pannello in alto a dx o usare il comando ls():\nls() ## [1] \u0026quot;altezza\u0026quot; \u0026quot;campione\u0026quot; \u0026quot;df\u0026quot; \u0026quot;fratelli\u0026quot; ## [5] \u0026quot;fuorisede\u0026quot; \u0026quot;lista1\u0026quot; \u0026quot;mat1\u0026quot; \u0026quot;mat2\u0026quot; ## [9] \u0026quot;mat3\u0026quot; \u0026quot;mat4\u0026quot; \u0026quot;mc_breakfast\u0026quot; \u0026quot;mc_data\u0026quot; ## [13] \u0026quot;newmat\u0026quot; \u0026quot;nome\u0026quot; \u0026quot;riga1\u0026quot; \u0026quot;riga2\u0026quot; ## [17] \u0026quot;var1\u0026quot; \u0026quot;var2\u0026quot; \u0026quot;var3\u0026quot; \u0026quot;vect1\u0026quot; ## [21] \u0026quot;vect2\u0026quot; \u0026quot;vect3\u0026quot; \u0026quot;vect4\u0026quot; \u0026quot;vect5\u0026quot; ## [25] \u0026quot;vect6\u0026quot; \u0026quot;vect7\u0026quot; \u0026quot;x\u0026quot; Per sapere dove stiamo salvando, bisogna capire in che directory stiamo lavorando, questo può essere fatto con il comando getwd():\ngetwd() ## [1] \u0026quot;/Users/tulliapadellini/blog/blog/content/post_ita\u0026quot; In questo caso il comando ci restituisce la cartella che abbiamo scelto quando abbiamo creato il progetto, possiamo però decidere di lavorare in un altra directory, per farlo usiamo il comando setwd():\nsetwd(\u0026#39;/Users/tulliapadellini/Dropbox (Personal)/ESERCITAZIONI ROMA III/LAB01\u0026#39;) Per salvare gli oggetti presenti nell’environment globale, il workspace, possiamo usare il comando save.image(“nome_del_file.RData”).\nsave.image(\u0026quot;workspace_lab01.RData\u0026quot;) Volendo possiamo anche non salvare tutti gli oggetti, ad esempio se vogliamo salvare solo vect1 e mat1 usiamo il comando:\nsave(vect1, mat1, file=\u0026#39;oggetti.RData\u0026#39;) In sostanza save.image() corrisponde a save(list=ls()).\nAl contrario per eliminare un elemento dal workspace usiamo il comando rm()\nrm(mat1) Verifichiamo con ls() che l’oggetto mat1 non è più presente nel workspace:\nls() ## [1] \u0026quot;altezza\u0026quot; \u0026quot;campione\u0026quot; \u0026quot;df\u0026quot; \u0026quot;fratelli\u0026quot; ## [5] \u0026quot;fuorisede\u0026quot; \u0026quot;lista1\u0026quot; \u0026quot;mat2\u0026quot; \u0026quot;mat3\u0026quot; ## [9] \u0026quot;mat4\u0026quot; \u0026quot;mc_breakfast\u0026quot; \u0026quot;mc_data\u0026quot; \u0026quot;newmat\u0026quot; ## [13] \u0026quot;nome\u0026quot; \u0026quot;riga1\u0026quot; \u0026quot;riga2\u0026quot; \u0026quot;var1\u0026quot; ## [17] \u0026quot;var2\u0026quot; \u0026quot;var3\u0026quot; \u0026quot;vect1\u0026quot; \u0026quot;vect2\u0026quot; ## [21] \u0026quot;vect3\u0026quot; \u0026quot;vect4\u0026quot; \u0026quot;vect5\u0026quot; \u0026quot;vect6\u0026quot; ## [25] \u0026quot;vect7\u0026quot; \u0026quot;x\u0026quot; Se vogliamo rimuovere tutti oggetti dal workspace usiamo:\nrm(list=ls()) # USARE CON CAUTELA ls() ## character(0) Oltre allo script e al workspace possiamo salvare l’elenco dei comandi eseguiti, la “history” della sessione.\nsavehistory(\u0026quot;history_lab01.Rhistory\u0026quot;) Come abbiamo visto prima, la history si può trovare anche nel pannello in alto a destra della finestra di R Studio. Per caricare history e workspace usiamo load():\nload(\u0026quot;workspace_lab01.RData\u0026quot;) loadhistory(\u0026quot;history_lab01.Rhistory\u0026quot;)  Opzionale - Funzioni —————————————————- Per quanto R abbia moltissime funzioni già “pronte per l’uso” (abbiamo visto log, mean, sd, etc), non sempre è possibile trovarne una che risponda alle nostre esigenze; è utile dunque essere in grado di costruire a mano delle funzioni.\nIn R una funzione si costruisce con il comando function(). Una generica funzione si definisce come segue:\nnome_della_funzione = function( argomento della funzione ) espressione della funzione Iniziamo con un esempio molto semplice, la funzione quadrato:\nquadrato = function(x) x^2 In questo caso x è l’argomento della funzione, mentre quadrato è il suo nome. L’espressione della funzione è l’argomento elevato al quadrato (^2).\nSe vogliamo determinare il quadrato di 5, è sufficiente richiamare la funzione specificandone l’argomento:\nquadrato(5) ## [1] 25 Naturalmente si ha anche che:\nquadrato(3) ## [1] 9 et cetera.\nConsideriamo ora una funzione di due elementi, x e a :\npotenza = function(x,a) x^a In questo caso la funzione consiste nel prendere il primo argomento ed elevarlo ad una potenza pari al secondo:\npotenza(2, 2) ## [1] 4 potenza(3, 2) ## [1] 9 potenza(3, 3) ## [1] 27   ","href":"/r-101/","title":"R - 101"},{"content":"  The intuition: Remember the (weak) Law of Large Numbers? Just a quick recap: if \\(X_1,\\dots, X_n\\) are i.i.d. random variables from a distribution \\(F\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the sample mean \\(\\bar{X}_n\\) converges in probability to the population mean \\(\\mu\\).\nUsing more formulas, we can write the LLN as \\[ \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\rightarrow \\mu =\\int xdF(x)\\] as \\(n\\rightarrow \\infty\\). One way to look at this is that, if the sample size \\(n\\) is “close to infinity” (or more generally, if \\(n\\) is “large enough”), then we have \\[\\frac{1}{n}\\sum_{i=1}^n X_i \\approx \\int xdF. \\]\nThis means that, if we are able to simulate an arbitrarily large amount of observations \\(X_i\\) from \\(F\\), then we can approximate arbitrarily well the mean \\(\\mu\\) of the distribution just by sample average.\nBut this is not all! The best part is that by the LLS (Law of the Lazy Statistician) we also have that \\[\\frac{1}{n}\\sum_{i=1}^n h(X_i) \\approx \\int h(x)dF .\\] In some sense, we are saying that if we are able to sample from the distribution \\(F\\), we can approximate arbitrarily complex integrals.\nYou may not see it straight away but this has huge consequences, such as: you may not ever have to solve an integral analytically again… Easy to see why LLN is an important result, right?\nUsing simulations from known random variables to approximate integrals is the core concept of the so-called Monte Carlo methods. In general, Monte Carlo methods are numerical techniques which rely on random sampling to approximate their results.\nIncidentally, Monte Carlo has a pretty interesting history, and dates back to von Neumann and the Manhattan Project (although some claims that the first Monte Carlo was used to solve the problem of Buffon’s needle almost \\(200\\) years before).\n Standard Integrals Monte Carlo is first and foremost one way of approximating integrals that we cannot (or do not want to) solve analytically. Consider for example: \\[ \\int^{2.5} _1 \\frac{\\sec x + \\tan x}{(\\sec x -\\tan x)(4\\tan ^2 x)^2} 8 \\sec ^2 x d x. \\]\nIf we can write this integral as an Expected Value then we can solve it using Monte Carlo. In actual fact it is pretty easy to do it.\nRemember that the density \\(f_U(x)\\) of a continuous uniform random variable \\(U\\sim(a,b)\\) is \\[f_U (x) = \\begin{cases} \\frac{1}{b-a} \u0026amp; x\\in[a,b]\\\\ 0 \u0026amp; \\text{otherwise} \\end{cases}\\]\nFor \\(a =1\\) and \\(b=2.5\\) we have: \\[f_U (x) = \\begin{cases} \\frac{1}{1.5} \u0026amp; x\\in[1,2.5]\\\\ 0 \u0026amp; \\text{otherwise} \\end{cases}\\]\nThis is enough to write the previous integral as an expectation: \\[ \\int^{2.5} _1 \\frac{\\sec x + \\tan x}{(\\sec x -\\tan x)(4\\tan ^2 x)^2} 8 \\sec ^2 x \\cdot 1\\cdot d x =\\] \\[ = \\int^{2.5} _1 \\frac{\\sec x + \\tan x}{(\\sec x -\\tan x)(4\\tan ^2 x)^2} 8 \\sec ^2 x \\cdot \\frac{1.5}{1.5}d x = \\] \\[ = \\int^{2.5} _1 \\frac{\\sec x + \\tan x}{(\\sec x -\\tan x)(4\\tan ^2 x)^2} 8 \\sec ^2 x \\cdot 1.5 f_U(x)d x = \\] \\[ = \\int^{2.5} _1 \\frac{\\sec x + \\tan x}{(\\sec x -\\tan x)(4\\tan ^2 x)^2} 8 \\sec ^2 x 1.5 d F_U(x) =\\] \\[ = \\int^{2.5} _1 h(x) d F_U(x) = \\mathbb{E}_{F_U}[h(X)] \\]\nh.funct = function(x){ num = 1/cos(x) + tan(x) denom = (1/cos(x) - tan(x))*(4*tan(x)^2)^2 num/denom * 8 * 1/cos(x) * 1.5 } h.funct(runif(1,1,2.5)) ## [1] -0.4072394 This function returns one observation from the random variable \\(Y = h(X)\\).\nIf we repeat the procedure \\(n\\) times, we get a sample \\(Y_1, \\dots, Y_n\\) from the distribution of \\(Y\\).\nn = 10000 y.sample = replicate(n, h.funct(runif(1, 1, 2.5))) y.average = mean(y.sample) y.average ## [1] -1.439108  “Strange” Transformations of Random Variables Suppose we are interested in the random variable \\[Y = \\min \\left\\{k \\text{ such that }\\sum_{i=1}^k u_i\u0026lt;1, \\quad u_i \\sim U(0,1)\\right\\} \\] in other words the variable defined as the minimum number \\(k\\) such that \\(\\sum_{i=1} ^k u_i \u0026lt; 1\\), where \\(u_1, \\dots, u_k\\) are i.i.d. random variables from a \\(Unif(0,1)\\). This is nothing but a function of a uniform random variable, so we can write it as \\(h(X)\\), where \\(X\\sim Unif(0,1)\\).\nWe are interested in the expected value of \\(Y\\), and it is difficult to compute it analytically. However if we use simulations, things become a lot easier.\nh.funct = function(){ s = 0 k = 0 while( s \u0026lt; 1){ u = runif(1) s = s+u k = k+1 } return(k) } The function returns one observation simulated from the random variable \\(Y\\). If we replicate the procedure \\(n\\) times, we get a sample \\(Y_1,\\dots,Y_n\\) distributed as \\(Y\\).\nn = 10000 y.sample = replicate(n, h.funct() )  We can use the sample average of this new sample to estimate the population mean of the variable \\(Y\\).\ny.average = mean(y.sample) y.average ## [1] 2.7278 Little side remark: Does it remind you of anything? It turns out that \\(\\mathbb{E}[Y]=e\\), so this is just another way to define Euler’s number.\nThis is good already, but we can do more! We have an entire sample \\(Y_1,\\dots,Y_n\\) generated from the unknown distribution of \\(Y\\), which we can use to carry on any kind of inference procedure (which is something useless at this point of the course but please keep it in mind, it will be extremely important later on!). As for now the only thing you can do is to analyse the distribution of \\(Y\\) looking at the histogram:\nhist(y.sample, col=rgb(.6,0,.4,.3))  Monte Carlo for the Coupon Problem As our last example, let us consider an exercise of basic probability, such as those you had to solve before Test::01.\nRemember Pokemon Go? Old stuff, time flies…\nThe idea behind the game is easy:\nwalk around encounter a Pokemon at random catch it  and then iterate the procedure until you had all of the 145 available Pokemon.\nDifferent Pokemon have different probability of appearing, so you could meet the same common Pokemon many many times before meeting a new rare one. Typically you would end up with a lot of Pidgeys and no Snorlaxes, and it might take a while to complete your pokedex (i.e. catch all the Pokemon). Before starting this battery-draining adventure, you might want to know: how long will it take to catch one for each kind of Pokemon? How many Pokemon do I need to encounter in order to end the game?\nIt turns out that this is a well known probability problem, so much so that it has its own name: the Coupon problem.\nLet us rewrite it a more rigorous way, and let us simplify the problem a little bit by assuming that all Pokemon appear with the same probability. Denote by \\(X_i\\) the number of encounters it takes for the player to find the \\(i-th\\) new Pokemon after the \\((i-1)\\)-th Pokemon has been caught. Clearly \\(\\mathbb{E}[X_1] =1\\) because the player starts with no Pokemon. After the \\((i-1)\\)-th Pokemon has been caught, there are \\(145 - (i-1)\\) possible Pokemon that could be the new \\(i\\)-th Pokemon. We can interpret the process of waiting for the new \\(i\\)-th Pokemon as a geometric distribution, where each trial is meeting a Pokemon, the “success” is getting any of the \\(n-(i-1)\\) unobserved Pokemon and “failure” is getting a duplicate of something we already have.\nWe can thus assume that \\(X_i- 1 \\sim Geom(p_i)\\), and use properties of the distribution to find that \\(\\mathbb{E}[X_i] =\\frac{1}{p_i}\\).\nRemember, we are interested in the number of encounters we need to capture all Pokemon \\(Y\\), which is the sum of the \\(X_i\\). In theory you could solve this problem with the tools that you already have (+ pen and paper), in fact, by linearity of the expected value, the problem consists in computing a bunch of expected values for Geometric distributions, i.e. \\[ \\mathbb{E}[Y] = \\mathbb{E}[X_1 +\\dots +X_{145}] = \\mathbb{E}[X_1] + \\dots \\mathbb{E}[X_{145}].\\]\nThe “tricky” part here is how to compute the parameter \\(p_i\\) of the Geometric distribution. If we assume that all the Pokemon have the same probability of appearing we have that, the probability of meeting the \\(i\\)-th new Pokemon, given that you already met \\((i-1)\\) distinct Pokemon is \\[p_i=\\frac{145-(i-1)}{145}.\\] In practice however, if we remove the oversimplifying assumption that all Pokemon have the same probability of appearing, computing this probability becomes rather long and tedious (although it is possible). Monte Carlo provides a fast (and almost brainless!) alternative. First thing first: load the data.\nlibrary(readr) spawn \u0026lt;- read_csv(\u0026quot;spawn-by-typ.csv\u0026quot;) ## Rows: 145 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## dbl (2): pokemon_id, appeareances ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Then define the sample space and the probability of encountering a Pokemon:\npokemon_sample_space = spawn$pokemon_id spawn_prob = spawn$appeareances Define a function that represent the encounter with a random Pokemon:\nfind.pokemon = function(){ sample(pokemon_sample_space, 1, replace=TRUE, prob = spawn_prob) } sum(spawn_prob) ## [1] 598427 Warning: spawn_prob is not a proper probability distribution, but it does not matter because sample standardizes it automatically.\nWe finally define a function that will give us the number of Pokemon that we need to meet before we have a number of distinct Pokemon equal to num_distinct_poke. This is our function \\(h\\).\nsimulate.catch.them.all \u0026lt;- function(num_distinct_poke){ captured \u0026lt;- c() while(length(unique(captured)) \u0026lt; num_distinct_poke){ captured \u0026lt;- c(captured, find.pokemon()) } length(captured) } Exactly as in the previous case, this function returns a random draw from the distribution of \\(Y\\); if we want to build the sample average we need, well, a sample, so we iterate the procedure.\nn = 1000 y.sample = replicate(n,simulate.catch.them.all(10)) y.average = mean(y.sample) y.average ## [1] 14.308 This means that in order to get 10 different Pokemon we need to run into 14.308 Pokemon.\n ","href":"/lectures/montecarlo/","title":"Basic Monte Carlo"},{"content":"","href":"/lectures/","title":"Lectures"},{"content":"where \u0026amp; when to find me:  address: stanza 39 \u0026ndash; Dipartimento di Scienze Statistiche, Piazzale Aldo Moro 5, 00185, Rome, ITALY office hours:  @Sapienza: tuesday 9 \u0026ndash; 11 @LUISS: by appointment     Teaching Last year I was teaching the Preparatory R Course for the Bachelor\u0026rsquo;s degree in Bioinformatics at Sapienza Università di Roma. Course material can be found here.\n I was also in charge of the Preparatory Course in Statistics for the Ms courses in Economics and Finance and Banking at Università di Roma - Torvergata. Course material is:\n Syllabus Descriptive Statistics Probability Statistical Inference   Old Material \u0026amp; Lecture notes (very much work in progress)\n [EN] - Some stuff covered in class [ITA] - Cose viste a lezione  ","href":"/teaching/","title":""},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/tags/config/","title":"Configuration"},{"content":"  {INLA\\ {as much as you can learn in 90 minutes}} INLA Efficient (i.e. fast) and accurate computational tool for Bayesian Statistics.\n INLA - the method  a deterministic algorithm to approximate the posterior distribution    R-INLA - the implementation  an R package to perform fit a large class of models in a Bayesian way    INLA -Integrated Nested Laplace Aproximation  it’s fast  relies on numerical approximation and sparse matrices    it’s accurate  empirically shows better performances than MCMC    it’s flexible  can be used to fit any model formulated as a GAN    it is (relatively) easy to use  it’s implemented as an R package    Motivation Do we actually need yet another way to implement Bayesian Methods?\nYes if we think that MCMC methods are\n cumbersome to write slow  And this is very much true when dealing with Spatial Models\n INLA vs MCMC take I \n INLA vs MCMC take II \n INLA INLA models \\[\\begin{align*} y|\\theta, \\psi \u0026amp;\\sim \\pi(y; \\theta, \\psi) \u0026amp; \u0026amp; \\text{Likelihood}\\\\ \\theta | \\psi \u0026amp;\\sim \\only\u0026lt;1\u0026gt;{\\pi(\\theta;\\psi)}\\only\u0026lt;2-3\u0026gt;{N(\\theta; 0, \\Sigma(\\psi) )} \u0026amp; \u0026amp; \\text{Latent structure }\\\\ \\psi \u0026amp;\\sim \\pi(\\psi) \u0026amp; \u0026amp; \\text{Hyperprior}\\\\ \\end{align*}\\]\nINLA provides numerical approximations of the marginal posteriors \\[\\pi(\\theta_i| y) \\qquad \\qquad \\pi(\\psi_j | y)\\]\nLinear models naturally fall in the INLA framework when we consider \\(\\theta = (\\beta, f_1, f_2, \\dots)\\) \\[y= k(\\eta) + \\epsilon \\qquad \\quad \\eta = x^t\\beta + \\sum_k f_k(z_k)\\] where \\(\\sum_k f_k(z_k)\\) can represent random effects, splines, anything you like.\n{ = N(x; 0, \\Sigma)}$ --  Laplace Approximation Laplace approximation is based on the following two key idea:\n\\[\\begin{align*} f(x) \u0026amp; = \\exp[\\log(f(x))] \\\\ g(x) \u0026amp; = g(x^*) + g\u0026#39;\u0026#39;(x^*)(x-x^*)^2 + \\text{error} \\approx g\u0026#39;\u0026#39;(x^*)(x-x^*)^2 \\end{align*}\\]\nSo that for every density \\(f\\) we have \\[f(x) \\approx \\exp[\\log(f)\u0026#39;\u0026#39; (x^*)(x-x^*)^2]\\]\nIntuitively we can approximate any density \\(f\\) with a Gaussian by:\n matching the mode to the mean of the Gaussian, \\(\\mu = x^*\\) setting the variance by looking at the curvaure at the mode \\(\\sigma = -1/\\log(f)\u0026#39;\u0026#39;(x^*)\\)   Basic INLA assumptions Each data point depends on only one of the elements in the latent Gaussian field \\(\\theta\\), the linear predictor  The size of the hyperparameter vector \\(\\psi\\) is small (say \u0026lt; 15)  The latent field \\(\\theta\\), can be large but it is endowed with some conditional independence (Markov) properties so that the precision matrix \\(\\Sigma^{-1}(\\psi)\\) is sparse.  The linear predictor depends linearly on the unknown smooth function of covariates.  The inferential interest lies in the univariate posterior marginals \\(\\pi(\\theta_i|y)\\) and \\(\\pi(\\psi_j|y)\\) rather than in the joint posterior \\(\\pi(\\theta, \\psi|y)\\).\n   INLA \\[\\begin{align*} \\pi(\\theta_i|y) \u0026amp; = \\int \\int \\pi(\\theta, \\psi|y) d\\theta_{-i} d\\psi = \\int \\textcolor\u0026lt;3-\u0026gt;{red}{\\pi(\\theta_{i}|\\psi,y)} \\textcolor\u0026lt;2-\u0026gt;{gray}{\\pi(\\psi|y)}d\\psi \\\\ \\only\u0026lt;2-\u0026gt;{\\widehat{\\pi}(\\theta_i|y) \u0026amp; = \\sum_{k} \\textcolor\u0026lt;3-\u0026gt;{red}{\\widehat{\\pi}(\\theta_{i}|\\psi^{(k)},y)} \\textcolor\u0026lt;2-\u0026gt;{gray}{\\widehat{\\pi}(\\psi^{(k)}|y)}\\Delta^{(k)}} \\end{align*}\\]\n  Approximate \\(\\textcolor{gray}{\\pi(\\psi|y)}\\) and \\(\\textcolor{red}{\\pi(\\theta_{i}|\\psi,y)}\\) through Laplace Approximation Approximate the integrals over \\(\\psi\\) with summations over a finite set of values \\(\\psi^{(1)},\\dots, \\psi^{(K)}\\)    Back to our Basic INLA assumptions Each data point depends on only one of the elements in the latent Gaussian field \\(\\theta\\), the linear predictor    The latent field \\(\\theta\\), can be large but it is endowed with some conditional independence (Markov) properties so that the precision matrix \\(\\Sigma^{-1}(\\psi)\\) is sparse.  The linear predictor depends linearly on the unknown smooth function of covariates.  \n   Posterior of \\(\\psi\\) \\[ \\pi(\\psi|y) = \\frac{\\pi(\\theta, \\psi|y)}{\\pi(\\theta | \\psi, y)} \\textcolor\u0026lt;1\u0026gt;{white}{\\propto \\frac{\\pi(y| \\theta, \\psi) \\pi(\\theta | \\psi) \\pi(\\psi)}{\\pi(\\theta | \\psi, y)}}\\]\nHere comes the Laplace approximation:\nApproximate \\(\\pi(\\theta | \\psi, y)\\) with a Gaussian \\(\\widehat{\\pi}_G(\\theta | \\psi, y) = N(\\theta; \\mu, Q^{-1})\\) wherer\n \\(\\mu\\) is the mode of \\(\\pi(\\theta | \\psi, y)\\) \\(-Q\\) is the curvature of \\(\\log[\\pi(\\theta | \\psi, y)]\\) at the mode \\(\\mu\\)  \\[ \\widehat{\\pi}(\\psi|y) = \\frac{\\pi(\\theta, \\psi|y)}{\\widehat{\\pi}_G(\\theta | \\psi, y)} {\\propto \\frac{\\pi(y| \\theta, \\psi) \\pi(\\theta | \\psi) \\pi(\\psi)}{\\widehat{\\pi}_G(\\theta | \\psi, y)}}\\]\n INLA \\[\\begin{align*} \\pi(\\theta_i|y) \u0026amp; = \\int \\int \\pi(\\theta, \\psi|y) d\\theta_{-i} d\\psi = \\int \\textcolor{red}{\\pi(\\theta_{i}|\\psi,y)} \\textcolor{gray}{\\pi(\\psi|y)}d\\psi \\end{align*}\\]\n Approximate \\(\\textcolor{gray}{\\pi(\\psi|y)}\\) and \\(\\textcolor{red}{\\pi(\\theta_{i}|\\psi,y)}\\) through Laplace Approximation Approximate the integrals over \\(\\psi\\) with summations over a set of carefully chosen values \\(\\psi^{(1)},\\dots, \\psi^{(K)}\\)  \\[\\begin{align*} \\widehat{\\pi}(\\theta_i|y) \u0026amp; = \\sum_{k} \\textcolor{red}{\\widehat{\\pi}(\\theta_{i}|\\psi^{(k)},y)} \\textcolor{gray}{\\widehat{\\pi}(\\psi^{(k)}|y)}\\Delta^{(k)} \\end{align*}\\]\n Approximate the Posterior Latent Field \\[ \\pi(\\theta_i|\\psi, y) = \\frac{\\pi(\\theta| \\psi, y)}{\\pi(\\theta_{-i} |\\theta_i, \\psi, y)} \\textcolor\u0026lt;1\u0026gt;{white}{\\propto \\frac{\\pi(y|\\theta, \\psi)\\pi(\\theta|\\psi)\\pi(\\psi)}{\\pi(\\theta_{-i} |\\theta_i, \\psi, y)}}\\]\n Gaussian: use the marginals of \\(\\widehat{\\pi}_G(\\theta | \\psi, y)\\) computed before Laplace approximation: use a Gaussian approximation for the denominator \\(\\pi(\\theta_{-i} |\\theta_i, \\psi, y)\\) Simplified Laplace approximation: a mix of the two   Putting everything together Explore the space of \\(\\psi\\) through the approximation \\(\\widehat{\\pi}(\\psi|y)\\).  Find the mode of \\(\\widehat{\\pi}(\\psi|y)\\) Select \\({\\psi^{(1)}, \\dots, \\psi^{(K)}}\\) in the area of high density of \\(\\widehat{\\pi}(\\psi|y)\\)   Compute \\(\\widehat{\\pi}(\\psi^{(k)}|y)\\) for each \\({\\psi^{(1)}, \\dots, \\psi^{(K)}}\\)  Compute \\(\\widehat{\\pi}(\\theta_i|\\psi^{(k)},y)\\) for each \\({\\psi^{(1)}, \\dots, \\psi^{(K)}}\\)  Approximate \\(\\pi(\\theta_i|y)\\) as \\[\\widehat{\\pi}(\\theta_i|y) = \\sum_{k} \\textcolor{red}{\\widehat{\\pi}(\\theta_{i}|\\psi^{(k)},y)} \\textcolor{gray}{\\widehat{\\pi}(\\psi^{(k)}|y)}\\Delta^{(k)}\\]\n    R-INLA Installation INLA is not on CRAN, so you need to specify the repository when you install it:\ninstall.packages(\u0026quot;INLA\u0026quot;, repos = \u0026quot;https://inla.r-inla-download.org/R/stable\u0026quot;, dep = TRUE) INLA gets constant updating - check your version\n Setting up the model The generic inla call is structured as follows:\ninla(formula, data, family)  formula: formula object that specifies the linear predictor\n data: data frame with the data\n family: string that indicate the likelihood family (default is Gaussian)\n   Toy Example The basic formulation of a linear regression model is almost the same as the canonical lm function:\nlibrary(INLA) data(iris) mod1 = inla(Petal.Length ~ 1 + Petal.Width, data = iris) mod1_lm = lm(Petal.Length ~ 1 + Petal.Width, data = iris)  The formula argument The formula object specifies the building blocks of the linear predictor\n\\[y= k(\\eta) + \\epsilon \\qquad \\quad \\eta = x^t\\beta + \\sum_k f_k(z_k)\\]\nformula = y ~ x + f(id, model) The f terms contains random effect\n id name of the variable model name of the model of the random effect corresponding to id   Toy Example formula = Petal.Length ~ 1 + Petal.Width + f(Species, model = \u0026quot;iid\u0026quot;) mod2= inla(formula, data = iris) NB: The list of all possible latent models can be found using:\nnames(inla.models()$latent) inla.doc(\u0026quot;ar1\u0026quot;)  The data argument Data are typically provided through a data.frame (although named list can also be used).\n If the response is a factor it must be converted to {0, 1} before calling inla(), as this conversion is not done automatic (as for example in glm()).  If the covariate is binary it has to be converted to a factor, otherwise inla will treat it as numeric  If we wish to predict the response variable for some observations, we need to specify the response variable of these observations as NA\n   The family argument The family argument is a string defining the likelihood of our model.\n each observation can have a different likelihood: vector of strings that indicate the likelihood family\n depending on the likelihood we are using, we may have additional arguments to provide to the inla() call\n  inla(formula, data, family = \u0026quot;binomial\u0026quot;, Ntrials)  we may have more than one link function corresponding to each family (as in the logit or probit case). control.family=list(control.link=list(model=\"model\")))  NB: The list of all possible likelihoods can be found using:\nnames(inla.models()$link)  Toy Example data(\u0026quot;Seeds\u0026quot;) res = inla(formula= r ~ x1 + x2, data = Seeds, family = \u0026quot;binomial\u0026quot;, Ntrials = n, control.family = list(control.link=list(model = \u0026quot;logit\u0026quot;))) summary(res) To see all available likelihood and links you can use:\nnames(inla.models()$link) names(inla.models()$likelihood)  Additional Arguments  control.compute: list with the specification of several computing variables such as dic which is a Boolean variable indicating whether the DIC of the model should be computed  res = inla(Petal.Length ~ 1 + Petal.Width, data = iris, control.compute = list(dic = TRUE))  control.predictor: list with the specification of several predictor variables such as link which is the link function of the model, and compute which is a Boolean variable that indicates whether the densities for the linear predictor should be computed.  res = inla(Petal.Length ~ 1 + Petal.Width, data = iris, control.predictor = list(compute = TRUE)) 1$ area $i$'s risk is higher than the average in the standard population. --  Even more additional arguments  inla.emarginal() and inla.qmarginal() calculate the expectation and quantiles, respectively, of the posterior marginals  inla.smarginal() can be used to obtain a spline smoothing of the whole marginal  inla.tmarginal() can be used to transform the marginals  inla.zmarginal() provides summary statistics  inla.dmarginal() computes the density at particular values\n    ","href":"/lectures/model/","title":"INLA"},{"content":"  INLA Efficient (i.e. fast) and accurate computational tool for Bayesian Statistics.\n INLA - the method  a deterministic algorithm to approximate the posterior distribution    R-INLA - the implementation  an R package to perform fit a large class of models in a Bayesian way    INLA -Integrated Nested Laplace Aproximation  it’s fast  relies on numerical approximation and sparse matrices    it’s accurate  empirically shows better performances than MCMC    it’s flexible  can be used to fit any model formulated as a GAN    it is (relatively) easy to use  it’s implemented as an R package    Motivation Do we actually need yet another way to implement Bayesian Methods?\nYes if we think that MCMC methods are\n cumbersome to write slow  And this is very much true when dealing with Spatial Models\n INLA vs MCMC take I GIBBS code VS INLA\n It’s slow time to run the two previous models\n INLA Formally \\[\\begin{align*} y|\\theta, \\psi \u0026amp;\\sim f(y; \\theta, \\psi) \u0026amp; \u0026amp; \\text{Likelihood}\\\\ \\theta | \\psi \u0026amp;\\sim \\only\u0026lt;1\u0026gt;{\\pi(\\theta;\\psi)}\\only\u0026lt;2-3\u0026gt;{N(\\theta; 0, \\Sigma(\\psi) )} \u0026amp; \u0026amp; \\text{Latent structure }\\\\ \\psi \u0026amp;\\sim \\pi(\\psi) \u0026amp; \u0026amp; \\text{Hyperprior}\\\\ \\end{align*}\\]\nINLA provides numerical approximations of the marginal posteriors \\[\\pi(\\theta_i| y) \\qquad \\qquad \\pi(\\psi_j | y)\\]\nLinear models naturally fall in the INLA framework when we consider \\(\\theta = (\\beta, f_1, f_2, \\dots)\\) \\[y= k(\\eta) + \\epsilon \\qquad \\quad \\eta = x^t\\beta + \\sum_k f_k(z_k)\\] where \\(\\sum_k f_k(z_k)\\) can represent random effects, splines, anything you like.\n{ = N(x; 0, \\Sigma)}$ --  INLA \u0026amp; Gaussianity You shouldn’t be surprised that INLA requires the assumption of Gaussianity, after all Laplace approximation is nothing but Gaussian approximation:\n match the mode to the mean set the variance by looking at the curvaure at the mode   Let us go into the details Each data point depends on only one of the elements in the latent Gaussian field \\(\\theta\\), the linear predictor, so that the likelihood can be written as\n The size of the hyperparameter vector \\(\\psi\\) is small (say \u0026lt; 15)\n The latent field \\(\\theta\\), can be large but it is endowed with some conditional independence (Markov) properties so that the precision matrix \\(\\Sigma^{-1}(\\psi)\\) is sparse.\n The linear predictor depends linearly on the unknown smooth function of covariates.\n The inferential interest lies in the univariate posterior marginals \\(\\pi(\\theta_i|y)\\) and \\(\\pi(\\psi_j|y)\\) rather than in the joint posterior \\(\\pi(\\theta, \\psi|y)\\).\n    \\[\\begin{align*} \\pi(\\theta_i|y) \u0026amp; = \\int \\int \\pi(\\theta, \\psi|y) d\\theta_{-i} d\\psi = \\int \\textcolor\u0026lt;3-\u0026gt;{red}{\\pi(\\theta_{i}|\\psi,y)} \\textcolor\u0026lt;2-\u0026gt;{gray}{\\pi(\\psi|y)}d\\psi \\\\ \\only\u0026lt;2-\u0026gt;{\\widetilde{\\pi}(\\theta_i|y) \u0026amp; = \\sum_{k} \\textcolor\u0026lt;3-\u0026gt;{red}{\\widehat{\\pi}(\\theta_{i}|\\psi^{(k)},y)} \\textcolor\u0026lt;2-\u0026gt;{gray}{\\widetilde{\\pi}(\\psi^{(k)}|y)}\\Delta^{(k)}} \\end{align*}\\]\n  Approximate \\(\\textcolor{gray}{\\pi(\\psi|y)}\\) and \\(\\textcolor{red}{\\pi(\\theta_{i}|\\psi,y)}\\) through Laplace Approximation Approximate the integrals over \\(\\psi\\) with summations over a finite set of values \\(\\psi^{(1)},\\dots, \\psi^{(K)}\\)    Back to our assumptions Each data point depends on only one of the elements in the latent Gaussian field \\(\\theta\\), the linear predictor, so that the likelihood can be written as  The latent field \\(\\theta\\), can be large but it is endowed with some conditional independence (Markov) properties so that the precision matrix \\(\\Sigma^{-1}(\\psi)\\) is sparse. The linear predictor depends linearly on the unknown smooth function of covariates.    Posterior of the hyperparameter \\[ \\pi(\\psi|y) = \\frac{\\pi(\\theta, \\psi|y)}{\\pi(\\theta | \\psi, y)} \\textcolor\u0026lt;1\u0026gt;{white}{\\propto \\frac{\\pi(y| \\theta, \\psi) \\pi(\\theta | \\psi) \\pi(\\psi)}{\\pi(\\theta | \\psi, y)}}\\]\nLaplace approximation:\nApproximate \\(\\pi(\\theta | \\psi, y)\\) with a Gaussian \\(\\widehat{\\pi}_G(\\theta | \\psi, y) = N(\\theta; \\theta^*, \\Sigma^*)\\) wherer\n \\(\\theta^*\\) is the mode of \\(\\pi(\\theta | \\psi, y)\\) \\(\\Sigma^*\\) is the curvature of \\(\\pi(\\theta | \\psi, y)\\) at the mode \\(\\theta^*\\)  \\[ \\widehat{\\pi}(\\psi|y) = \\frac{\\pi(\\theta, \\psi|y)}{\\widehat{\\pi}_G(\\theta | \\psi, y)} {\\propto \\frac{\\pi(y| \\theta, \\psi) \\pi(\\theta | \\psi) \\pi(\\psi)}{\\widehat{\\pi}_G(\\theta | \\psi, y)}}\\]\n Reprise \\[\\begin{align*} \\pi(\\theta_i|y) \u0026amp; = \\int \\int \\pi(\\theta, \\psi|y) d\\theta_{-i} d\\psi = \\int \\textcolor{red}{\\pi(\\theta_{i}|\\psi,y)} \\textcolor{gray}{\\pi(\\psi|y)}d\\psi \\end{align*}\\]\n Approximate \\(\\textcolor{gray}{\\pi(\\psi|y)}\\) and \\(\\textcolor{red}{\\pi(\\theta_{i}|\\psi,y)}\\) through Laplace Approximation Approximate the integrals over \\(\\psi\\) with summations over a set of carefully chosen values \\(\\psi^{(1)},\\dots, \\psi^{(K)}\\)  \\[\\begin{align*} \\widetilde{\\pi}(\\theta_i|y) \u0026amp; = \\sum_{k} \\textcolor{red}{\\widehat{\\pi}(\\theta_{i}|\\psi^{(k)},y)} \\textcolor{gray}{\\widetilde{\\pi}(\\psi^{(k)}|y)}\\Delta^{(k)} \\end{align*}\\]\n Approximate the Posterior Latent Field \\[ \\pi(\\theta_i|\\psi, y) = \\frac{\\pi(\\theta| \\psi, y)}{\\pi(\\theta_{-i} |\\theta_i, \\psi, y)} \\textcolor\u0026lt;1\u0026gt;{white}{\\propto \\frac{\\pi(y|\\theta, \\psi)\\pi(\\theta|\\psi)\\pi(\\psi)}{\\pi(\\theta_{-i} |\\theta_i, \\psi, y)}}\\]\n Gaussian: use the marginals of \\(\\widehat{\\pi}_G(\\theta | \\psi, y)\\) computed before Laplace approximation: use a Gaussian approximation for the denominator \\(\\pi(\\theta_{-i} |\\theta_i, \\psi, y)\\) Simplified Laplace approximation: a mix of the two   Putting everything together Explore the space of \\(\\psi\\) through the approximation \\(\\widehat{\\pi}(\\psi|y)\\).  Find the mode of \\(\\widehat{\\pi}(\\psi|y)\\) Select \\({\\psi^{(1)}, \\dots, \\psi^{(K)}}\\) in the area of high density of \\(\\widehat{\\pi}(\\psi|y)\\)  Compute \\(\\widehat{\\pi}(\\psi^{(k)}|y)\\) for each \\({\\psi^{(1)}, \\dots, \\psi^{(K)}}\\) Compute \\(\\widehat{\\pi}(\\theta_i|\\psi^{(k)},y)\\) for each \\({\\psi^{(1)}, \\dots, \\psi^{(K)}}\\) Approximate \\(\\pi(\\theta_i|y)\\) as \\[\\widetilde{\\pi}(\\theta_i|y) = \\sum_{k} \\textcolor{red}{\\widehat{\\pi}(\\theta_{i}|\\psi^{(k)},y)} \\textcolor{gray}{\\widetilde{\\pi}(\\psi^{(k)}|y)}\\Delta^{(k)}\\]    R-INLA The practice INLA is not on CRAN, so you need to specify the repository when you install it:\ninstall.packages(\u0026quot;INLA\u0026quot;, repos = \u0026quot;https://inla.r-inla-download.org/R/stable\u0026quot;, dep = TRUE)  Setting up a model The generic INLA call is structured as follows:\ninla(formula, data, family)  formula: formula object that specifies the linear predictor\n data: data frame with the data\n family: string that indicate the likelihood family (default is Gaussian)\n   Basic Example library(INLA) data(iris) mod1 = inla(Petal.Length ~ 1 + Petal.Width, data = iris) mod1_lm = lm(Petal.Length ~ 1 + Petal.Width, data = iris)  formula The formula object specifies the building blocks of the linear predictor\n\\[y= k(\\eta) + \\epsilon \\qquad \\quad \\eta = x^t\\beta + \\sum_k f_k(z_k)\\]\nformula = y ~ x + f(id, model) The f terms contains random effect\n id name of the variable model name of the model of the random effect corresponding to id   Example formula = Petal.Length ~ 1 + Petal.Width + f(Species, model = \u0026quot;iid\u0026quot;) mod2= inla(formula, data = iris) The list of all possible latent models can be found using:\nnames(inla.models()$latent) inla.doc(\u0026quot;ar1\u0026quot;)  Data Data are typically provided through a data.frame (although named lists can also be used).\n If the response is a factor it must be converted to {0, 1} before calling inla(), as this conversion is not done automatic (as for example in glm()).\n If the covariate is binary it has to be converted to a factor, otherwise inla will treat it as numeric\n If we wish to predict the response variable for some observations, we need to specify the response variable of these observations as NA\n   Family The family argument is a string defining the likelihood of our model.\n each observation can have a different likelihood: vector of strings that indicate the likelihood family\n depending on the likelihood we are using, we may have additional arguments to provide to the inla() call\n  inla(formula, data, family = \u0026quot;binomial\u0026quot;, Ntrials)  we may have more than one link function corresponding to each family (as in the logit or probit case). control.family=list(control.link=list(model=“model”)))  names(inla.models()$link)\nBy default family is gaussian. A list of possible alternatives can be seen by typing names(inla.models()$likelihood), and details for individual families can be seen with inla.doc(“familyname”)\n Example data(\u0026quot;Seeds\u0026quot;) res = inla(formula= r ~ x1 + x2, data = Seeds, family = \u0026quot;binomial\u0026quot;, Ntrials = n, control.family = list(control.link=list(model = \u0026quot;logit\u0026quot;))) summary(res) To see all available likelihood and links you can use:\nnames(inla.models()$link) names(inla.models()$likelihood)  Additional Arguments  control.compute: list with the specification of several computing variables such as dic which is a Boolean variable indicating whether the DIC of the model should be computed  res = inla(Petal.Length ~ 1 + Petal.Width, data = iris, control.compute = list(dic = TRUE))  control.predictor: list with the specification of several predictor variables such as link which is the link function of the model, and compute which is a Boolean variable that indicates whether the marginal densities for the linear predictor should be computed.  res = inla(Petal.Length ~ 1 + Petal.Width, data = iris, control.predictor = list(compute = TRUE))  YOUR TURN  Using the DIC for comparison assess whether adding a random effect for the species type is beneficial for iris Data\n Use\n   Scottish Lip cancer \\[y_i|\\theta_i \\sim \\text{Poisson}(E_i\\theta_i)\\]\n \\(y_i\\) number of observed cases in area \\(i\\)\n \\(E_i\\) expected number of cases in area \\(i\\) \\(\\theta_i\\) is the relative risk. If \\(\\theta_i\u0026gt;1\\) area \\(i\\)’s risk is higher than the average in the standard population.   \\(X_i\\)  \\[\\log \\theta_i = \\beta_0+\\beta_1X_i\\]\n CAR Conditional Autoregressive models\n   inla.emarginal() and inla.qmarginal() calculate the expectation and quantiles, respectively, of the posterior marginals. inla.smarginal() can be used to obtain a spline smoothing, inla.tmarginal() can be used to transform the marginals, and inla.zmarginal() provides summary statistics. inla.dmarginal() computes the density at particular values. For example, the density at value -2.5 can be computed as follows (Figure 4.3):    ","href":"/lectures/untitled/","title":"INLA"},{"content":"","href":"/tags/og/","title":"Opengraph"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/series/","title":"Series"},{"content":"","href":"/tags/","title":"Tags"}]
